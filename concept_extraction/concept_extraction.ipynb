{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing graph and path building strategies\n",
    "\n",
    "1. Get material from wikipedia articles.\n",
    "2. Extract concepts\n",
    " - after part of speech tagging, chunk different types of concepts. make probable_lists and sentence classifications. \n",
    "3. Build undirected graph of concepts. Graph conveys centrality of concepts, and the existence of a relationship between two concepts. \n",
    "4. Get student's input on which concepts are not at all known\n",
    "___________\n",
    "5. Side note: maybe build a directed graph from the undirected graph. annotate sentence types and calculate readability. Then, look for patterns to learn how to classify prerequisites from this. \n",
    "_______\n",
    "\n",
    "Evaluating different metrics for building directed prerequisite concepts graph:\n",
    "\n",
    "1. term frequency\n",
    "2. inverse document frequency for several corpii:\n",
    "    wiki corpus \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Downloading a wikipedia article's text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'https://en.wikipedia.org/wiki/Facet'\n",
    "\n",
    "source = requests.get(url).text\n",
    "soup = BeautifulSoup(source,'lxml')\n",
    "\n",
    "\n",
    "text_set = soup.find_all(['p'])\n",
    "text_list = [p1.get_text() for p1 in text_set]\n",
    "tags_list = [p1.name for p1 in text_set ]\n",
    "\n",
    "rawtxt = ''.join(text_list)\n",
    "## This will skip headings ('h2','h3') and lists that are made as links( 'li'). For now, this is okay.\n",
    "print(\"length of material\")\n",
    "print(len(rawtxt))\n",
    "\n",
    "print(\"Sample of text\")\n",
    "print(rawtxt[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save rawtxt as is for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_name = \"C:/Users/Arati/Documents/personal docs/python_introduction_course/textdata/\"\n",
    "with open(path_name + filename,\"a\",encoding=\"utf-8\") as myfile:\n",
    "    myfile.write(rawtxt)\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternately getting file from disk and loading to rawtxt:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Cognitive_Load_Theory.txt'\n",
    "path_name = \"C:/Users/Arati/Documents/personal docs/python_introduction_course/textdata/\"\n",
    "with open (path_name +filename, \"r\",encoding=\"utf-8\") as myfile:\n",
    "    rawtxt=myfile.read()\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting list of concepts:\n",
    "\n",
    "### 2.1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.chunk import *\n",
    "from nltk.chunk.util import *\n",
    "from nltk.chunk.regexp import *\n",
    "from nltk import Tree\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "import itertools\n",
    "from itertools import chain\n",
    "import collections\n",
    "import numpy as num\n",
    "import pandas as pd\n",
    "import csv\n",
    "import statistics\n",
    "from nltk.corpus import cmudict\n",
    "cmud = cmudict.dict()\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "with open('common_words_documents.pickle', 'rb') as f:\n",
    "       common_words_documents = pickle.load(f)\n",
    "f.close()\n",
    "type(common_words_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training an unsupervised sentence tokenizer based off downloaded material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in text 269\n",
      "269\n",
      "Sample of sentences:\n",
      "['\\ufeffCatalysis (/kəˈtæləsɪs/) is the process of increasing the rate of a chemical reaction by adding a substance known as a catalyst[1] (/ˈkætəlɪst/), which is not consumed in the catalyzed reaction and can continue to act repeatedly. Because of this, only very small amounts of catalyst are required to alter the reaction rate in principle.', '[2]\\nIn general, chemical reactions occur faster in the presence of a catalyst because the catalyst provides an alternative reaction pathway with a lower activation energy than the non-catalyzed mechanism.', 'In catalyzed mechanisms, the catalyst usually reacts to form a temporary intermediate, which then regenerates the original catalyst in a cyclic process.', 'A substance which provides a mechanism with a higher activation energy does not decrease the rate because the reaction can still occur by the non-catalyzed route.', '[3] An added substance which does reduce the reaction rate is not considered a catalyst[1] but a reaction inhibitor (see below).']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(rawtxt)\n",
    " \n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "sents = tokenizer.tokenize(rawtxt)\n",
    "\n",
    "print(\"Number of sentences in text \"+str(len(sents)))\n",
    "print(len(sents))\n",
    "\n",
    "print(\"Sample of sentences:\")\n",
    "print(sents[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Extracting concepts from text, sentence by sentence\n",
    "\n",
    "#### 2.3.1 Setting up chunking rules:\n",
    "\n",
    "Chunking has to be done in batches. this way single noun concepts can be extracted as well as adjectives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkrules = {}\n",
    "\n",
    "chunkrules['JJNP'] = r\"\"\"    \n",
    "    JJNP: {<RB.*>?<J.*>?<NN.*>{1,}}       \n",
    "\"\"\"\n",
    "## Examples: reusable contactless stored value smart card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validchar function - checks if it is alphanumeric including hyphens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validchar(wrd):\n",
    "    if wrd.isalnum():\n",
    "        ##print('isalnum')\n",
    "        return 1\n",
    "    elif '-' in wrd:\n",
    "        wrd = wrd.replace('-','1')\n",
    "        if wrd.isalnum():\n",
    "            ##print('only replaced hyphens')\n",
    "            return 1\n",
    "        else:\n",
    "           ## print('replaced hyphens but still not alnum')\n",
    "            return 0\n",
    "            \n",
    "    else:\n",
    "       ## print('there were no hyphens')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_by_pos(tag):\n",
    "    token = tag[0]\n",
    "    pos = tag[1]\n",
    "    if token in stop_words:\n",
    "        return (token,pos)\n",
    "    if pos.startswith('J'):\n",
    "        # adjective form\n",
    "        lemma = wnl.lemmatize(token,'s')\n",
    "    elif pos.startswith('N'):\n",
    "        # noun form\n",
    "        lemma = wnl.lemmatize(token,'n')\n",
    "    elif pos.startswith('R'):\n",
    "        # adverb\n",
    "        lemma = wnl.lemmatize(token,'r')\n",
    "    elif pos.startswith('V'):\n",
    "        lemma = wnl.lemmatize(token,'v')\n",
    "    else:\n",
    "        lemma = token\n",
    "    return (lemma,pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dance', 'VBS')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_by_pos(('dancing','VBS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chunk this function = takes a grammar rule and sentence tags and returns chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_this(grammar_rule_key,sentence_tags):\n",
    "    setlist = []\n",
    "    cp = nltk.RegexpParser(chunkrules[grammar_rule_key])\n",
    "    J = cp.parse(sentence_tags) \n",
    "    for i in range(len(J)):\n",
    "        if not(isinstance(J[i],tuple)):\n",
    "            if (J[i].label()==grammar_rule_key):\n",
    "                setlist.append((' '.join([J[i][j][0] for j in range(len(J[i])) if (validchar(J[i][j][0])==1)])))\n",
    "    setlist = list(set(setlist))\n",
    "    setlist = [wrd.lower() for wrd in setlist if len(wrd)>0]\n",
    "    return setlist\n",
    "#%% \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an eqn dictionary to annotate mathy stuff out and use as noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "global eqn_dict\n",
    "eqn_dict = {}\n",
    "global eqn_count\n",
    "eqn_count = 1\n",
    "\n",
    "def eqn_label(tokens):\n",
    "    global eqn_count\n",
    "    global eqn_dict\n",
    "    EQNlist = [wrd for wrd in tokens if not(wrd.isalnum()) and re.search(r'[\\[\\]\\{\\}\\+*^=_%$]',wrd) and len(wrd)>1 ]\n",
    "    ## replace queations with a label and save to equation dictionary\n",
    "    for eqn in EQNlist:\n",
    "        \n",
    "        if not(eqn in eqn_dict):\n",
    "            \n",
    "            eqn_dict[eqn] = ''.join(['equation',str(eqn_count)])\n",
    "            eqn_count = eqn_count + 1                          \n",
    "        else:    \n",
    "            tokens[tokens.index(eqn)] = eqn_dict[eqn]\n",
    "                  \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "global inv_eqn_dict\n",
    "inv_eqn_dict = dict([[value,key] for key,value in eqn_dict.items()])\n",
    "\n",
    "def display_equation(reptokens):\n",
    "    for wrd in reptokens:\n",
    "        if wrd in inv_eqn_dict:\n",
    "            reptokens[reptokens.index(wrd)] = inv_eqn_dict[wrd]\n",
    "    return reptokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(sentence_tags):\n",
    "    return [chunk_this(key,sentence_tags)  for key in chunkrules]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will be stemming before saving sents_to_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "sent_to_np = {}\n",
    "sent_to_tokens = {}\n",
    "sent_to_tags = {}\n",
    "\n",
    "# tokens = [word_tokenize(s) for s in sents]\n",
    "# reptokens = [eqn_label(t) for t in tokens]\n",
    "# tags = [nltk.pos_tag(rt) for rt in reptokens]\n",
    "# sent_to_np = {s:}\n",
    "\n",
    "for i in range(len(sents)):\n",
    "    tokens = word_tokenize(sents[i])\n",
    "    reptokens = eqn_label(tokens)\n",
    "    tags = nltk.pos_tag(reptokens)\n",
    "    #print(tags)\n",
    "    lemmatags = [lemmatize_by_pos(t) for t in tags]\n",
    "    #print(lemmatags)\n",
    "    sent_to_np[i] = chunker(lemmatags)\n",
    "    ##sent_to_np_basics = extract_basicnp(sent_to_np[i])\n",
    "    #sent_to_tokens[i] = lemma\n",
    "    #sent_to_tags[i] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['station rf impedance', 'shd', 'ccp discharge how', 'pecvd q', 'a pecvd']]\n"
     ]
    }
   ],
   "source": [
    "print(sent_to_np[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_to_npflat = {}\n",
    "np_to_sent = {}\n",
    "for key in sent_to_np:\n",
    "    sent_to_npflat[key] = list(set((itertools.chain(*sent_to_np[key]))))  \n",
    "    for np in sent_to_npflat[key]:            \n",
    "        if np in np_to_sent:                           \n",
    "            np_to_sent[np].append(key)\n",
    "        else:                \n",
    "            np_to_sent[np]=[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np1 always appears before np2\n",
    "\n",
    "def build_graph(sent_to_npflat,max_sent_dist, min_sent_dist):\n",
    "    npnp_bondstrengthdir = {}\n",
    "    for i in range(len(sent_to_npflat)-max_sent_dist):\n",
    "        for np1 in sent_to_npflat[i]:\n",
    "            npnp_bondstrengthdir[np1] = {}\n",
    "            for j in range(min_sent_dist, max_sent_dist):\n",
    "                np2list = [np2 for np2 in sent_to_npflat[i+j] if np2!=np1]\n",
    "                for np2 in np2list:\n",
    "                    npnp_bondstrengthdir[np1][np2] =npnp_bondstrengthdir[np1].get(np2,0) + 1/(j+1)\n",
    "    return npnp_bondstrengthdir\n",
    "\n",
    "npnp_bondstrengthdir = build_graph(sent_to_npflat,3,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccp discharge how station rf impedance\n",
      "ccp discharge how pecvd q\n",
      "ccp discharge how shd\n",
      "ccp discharge how a pecvd\n",
      "ccp discharge how series rf circuit\n",
      "ccp discharge how simple model assumes\n",
      "ccp discharge how gas molecule\n",
      "ccp discharge how resistive impedance\n",
      "ccp discharge how collision\n",
      "ccp discharge how real part\n",
      "station rf impedance ccp discharge how\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for np1 in npnp_bondstrengthdir.keys():\n",
    "    for np2 in npnp_bondstrengthdir[np1].keys():\n",
    "        print(np1,np2)\n",
    "        count = count+1\n",
    "        if count>10:\n",
    "            break\n",
    "    if count>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuters IDF dictionary has already been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(np,rawtxt):\n",
    "   p = re.compile(np)\n",
    "   return len(p.findall(rawtxt))        \n",
    "\n",
    "wnl = nltk.WordNetLemmatizer()   \n",
    "def reuters_idf(token):\n",
    "    ## assuming np1list contains the np1 words for the sentence under consideration.\n",
    "    if wnl.lemmatize(token) in common_words_documents:\n",
    "           idf = math.log(10788) - math.log((1+common_words_documents[wnl.lemmatize(token)]))\n",
    "    else:\n",
    "           idf = math.log(10788)\n",
    "    return idf\n",
    "\n",
    "def tf_reuters_idf(np,rawtxt):\n",
    "   return tf(np,rawtxt)*reuters_idf(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print('prefixes')\n",
    "# example = 'dancer'\n",
    "# print(wnl.lemmatize(example))\n",
    "# print(porter.stem(example))\n",
    "# print(lancaster.stem(example))\n",
    "\n",
    "# print('prefix2')\n",
    "# example = 'dances'\n",
    "# print(wnl.lemmatize(example))\n",
    "# print(porter.stem(example))\n",
    "# print(lancaster.stem(example))\n",
    "\n",
    "# print('prefix3')\n",
    "# example = 'dancers'\n",
    "# print(wnl.lemmatize(example))\n",
    "# print(porter.stem(example))\n",
    "# print(lancaster.stem(example))\n",
    "\n",
    "# print('prefix4')\n",
    "# example = 'dancing'\n",
    "# print(wnl.lemmatize(example))\n",
    "# print(porter.stem(example))\n",
    "# print(lancaster.stem(example))\n",
    "\n",
    "# there seems to be no hard and fast rules for hte behaviour of the porter stemmer.\n",
    "# lemmatizer is slowest. \n",
    "# lemmatizer always solves plural to singular\n",
    "# examples = leaves, boxes, machines, areas: \n",
    "# prefixes for antonyms - word is retained as is for lemmatizer, porter and lancaster reduce to stems without getting rid of prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Concept = pd.Series([key for (key,value) in np_to_sent.items()])\n",
    "Occurence = pd.Series([num.array(value) for (key,value) in np_to_sent.items()])\n",
    "Frequency = pd.Series([len(o) for o in Occurence])\n",
    "Sdev = pd.Series([num.std(o) for o in Occurence])\n",
    "#ReutersIDF = pd.Series([reuters_idf(key) for (key,value) in np_to_sent.items()])\n",
    "Conceptdata = pd.DataFrame({'Concept':Concept,'Occurence':Occurence,'Frequency':Frequency,'Sdev':Sdev})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Occurence</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Sdev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>plasma</td>\n",
       "      <td>[102, 107, 112, 113, 114, 137, 156, 172, 190, ...</td>\n",
       "      <td>30</td>\n",
       "      <td>230.600434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>ion</td>\n",
       "      <td>[27, 70, 99, 111, 128, 129, 131, 134, 135, 136...</td>\n",
       "      <td>26</td>\n",
       "      <td>237.706360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>surface</td>\n",
       "      <td>[44, 78, 80, 82, 88, 102, 147, 162, 163, 208, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>218.631402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>lf</td>\n",
       "      <td>[15, 21, 131, 197, 199, 201, 287, 292, 305, 36...</td>\n",
       "      <td>20</td>\n",
       "      <td>180.389377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>radical</td>\n",
       "      <td>[69, 78, 79, 87, 89, 149, 151, 209, 218, 224, ...</td>\n",
       "      <td>20</td>\n",
       "      <td>120.784219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>wafer</td>\n",
       "      <td>[19, 125, 203, 254, 268, 307, 335, 411, 448, 4...</td>\n",
       "      <td>19</td>\n",
       "      <td>220.698727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>process</td>\n",
       "      <td>[29, 33, 55, 98, 150, 169, 303, 317, 318, 390,...</td>\n",
       "      <td>19</td>\n",
       "      <td>207.593786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>sheath</td>\n",
       "      <td>[34, 72, 101, 103, 106, 109, 127, 128, 129, 13...</td>\n",
       "      <td>18</td>\n",
       "      <td>142.286834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>showerhead</td>\n",
       "      <td>[52, 203, 268, 407, 420, 461, 497, 500, 540, 5...</td>\n",
       "      <td>17</td>\n",
       "      <td>169.368642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>station</td>\n",
       "      <td>[176, 209, 297, 406, 484, 485, 487, 489, 492, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>114.125712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>electron</td>\n",
       "      <td>[9, 27, 67, 70, 111, 113, 114, 127, 137, 157, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>245.576506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>plasma density</td>\n",
       "      <td>[11, 77, 97, 104, 164, 191, 281, 284, 296, 311...</td>\n",
       "      <td>16</td>\n",
       "      <td>189.527115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>example</td>\n",
       "      <td>[26, 35, 160, 189, 291, 302, 305, 387, 402, 47...</td>\n",
       "      <td>16</td>\n",
       "      <td>187.804045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>gap</td>\n",
       "      <td>[67, 173, 179, 291, 313, 314, 315, 318, 326, 3...</td>\n",
       "      <td>15</td>\n",
       "      <td>173.872239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>hf</td>\n",
       "      <td>[133, 197, 199, 200, 201, 292, 385, 386, 425, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>156.536509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>power</td>\n",
       "      <td>[59, 238, 283, 314, 371, 393, 406, 460, 479, 4...</td>\n",
       "      <td>14</td>\n",
       "      <td>150.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>ground</td>\n",
       "      <td>[16, 46, 124, 260, 267, 268, 273, 492, 493, 51...</td>\n",
       "      <td>13</td>\n",
       "      <td>195.207794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>result</td>\n",
       "      <td>[15, 102, 134, 167, 216, 299, 463, 472, 489, 5...</td>\n",
       "      <td>12</td>\n",
       "      <td>221.488462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>shd-ped gap</td>\n",
       "      <td>[4, 185, 309, 310, 316, 319, 325, 555, 643, 64...</td>\n",
       "      <td>12</td>\n",
       "      <td>206.957457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>electrode</td>\n",
       "      <td>[5, 37, 41, 42, 156, 166, 211, 246, 276, 495, ...</td>\n",
       "      <td>12</td>\n",
       "      <td>219.104031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bulk plasma</td>\n",
       "      <td>[3, 4, 5, 6, 72, 94, 95, 99, 101, 109, 126]</td>\n",
       "      <td>11</td>\n",
       "      <td>47.237224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>discharge</td>\n",
       "      <td>[3, 56, 164, 203, 246, 344, 398, 486, 514, 537...</td>\n",
       "      <td>11</td>\n",
       "      <td>211.011104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>effect</td>\n",
       "      <td>[44, 144, 148, 150, 213, 246, 301, 497, 615, 6...</td>\n",
       "      <td>11</td>\n",
       "      <td>231.943317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>pressure</td>\n",
       "      <td>[59, 92, 185, 191, 192, 365, 371, 393, 395, 54...</td>\n",
       "      <td>11</td>\n",
       "      <td>175.140086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>ignition</td>\n",
       "      <td>[53, 54, 55, 60, 62, 174, 220, 348, 486, 666]</td>\n",
       "      <td>10</td>\n",
       "      <td>205.269969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Concept                                          Occurence  \\\n",
       "324          plasma  [102, 107, 112, 113, 114, 137, 156, 172, 190, ...   \n",
       "85              ion  [27, 70, 99, 111, 128, 129, 131, 134, 135, 136...   \n",
       "136         surface  [44, 78, 80, 82, 88, 102, 147, 162, 163, 208, ...   \n",
       "47               lf  [15, 21, 131, 197, 199, 201, 287, 292, 305, 36...   \n",
       "235         radical  [69, 78, 79, 87, 89, 149, 151, 209, 218, 224, ...   \n",
       "63            wafer  [19, 125, 203, 254, 268, 307, 335, 411, 448, 4...   \n",
       "91          process  [29, 33, 55, 98, 150, 169, 303, 317, 318, 390,...   \n",
       "107          sheath  [34, 72, 101, 103, 106, 109, 127, 128, 129, 13...   \n",
       "166      showerhead  [52, 203, 268, 407, 420, 461, 497, 500, 540, 5...   \n",
       "486         station  [176, 209, 297, 406, 484, 485, 487, 489, 492, ...   \n",
       "30         electron  [9, 27, 67, 70, 111, 113, 114, 127, 137, 157, ...   \n",
       "37   plasma density  [11, 77, 97, 104, 164, 191, 281, 284, 296, 311...   \n",
       "83          example  [26, 35, 160, 189, 291, 302, 305, 387, 402, 47...   \n",
       "227             gap  [67, 173, 179, 291, 313, 314, 315, 318, 326, 3...   \n",
       "384              hf  [133, 197, 199, 200, 201, 292, 385, 386, 425, ...   \n",
       "196           power  [59, 238, 283, 314, 371, 393, 406, 460, 479, 4...   \n",
       "53           ground  [16, 46, 124, 260, 267, 268, 273, 492, 493, 51...   \n",
       "51           result  [15, 102, 134, 167, 216, 299, 463, 472, 489, 5...   \n",
       "20      shd-ped gap  [4, 185, 309, 310, 316, 319, 325, 555, 643, 64...   \n",
       "21        electrode  [5, 37, 41, 42, 156, 166, 211, 246, 276, 495, ...   \n",
       "11      bulk plasma        [3, 4, 5, 6, 72, 94, 95, 99, 101, 109, 126]   \n",
       "16        discharge  [3, 56, 164, 203, 246, 344, 398, 486, 514, 537...   \n",
       "138          effect  [44, 144, 148, 150, 213, 246, 301, 497, 615, 6...   \n",
       "198        pressure  [59, 92, 185, 191, 192, 365, 371, 393, 395, 54...   \n",
       "167        ignition      [53, 54, 55, 60, 62, 174, 220, 348, 486, 666]   \n",
       "\n",
       "     Frequency        Sdev  \n",
       "324         30  230.600434  \n",
       "85          26  237.706360  \n",
       "136         23  218.631402  \n",
       "47          20  180.389377  \n",
       "235         20  120.784219  \n",
       "63          19  220.698727  \n",
       "91          19  207.593786  \n",
       "107         18  142.286834  \n",
       "166         17  169.368642  \n",
       "486         17  114.125712  \n",
       "30          17  245.576506  \n",
       "37          16  189.527115  \n",
       "83          16  187.804045  \n",
       "227         15  173.872239  \n",
       "384         14  156.536509  \n",
       "196         14  150.984000  \n",
       "53          13  195.207794  \n",
       "51          12  221.488462  \n",
       "20          12  206.957457  \n",
       "21          12  219.104031  \n",
       "11          11   47.237224  \n",
       "16          11  211.011104  \n",
       "138         11  231.943317  \n",
       "198         11  175.140086  \n",
       "167         10  205.269969  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conceptdata.sort_values(by='Frequency',ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conceptdata.to_csv('PlasmafaqConceptdata.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect as bs\n",
    "\n",
    "def find_shortest_distance(search_list, value):\n",
    "    ins_point = bs.bisect_right(search_list,value)\n",
    "    if ins_point < len(search_list):\n",
    "        return min(abs(search_list[ins_point] - value), abs(search_list[ins_point - 1] - value))\n",
    "    return abs(search_list[ins_point - 1] - value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_shortest_distance_withdir(search_list,value):\n",
    "    ins_point = bs.bisect_right(search_list, value)\n",
    "    if ins_point < len(search_list):\n",
    "        if abs(search_list[ins_point] - value) < abs(search_list[ins_point - 1] - value):\n",
    "            return search_list[ins_point] - value\n",
    "        else:\n",
    "            return search_list[ins_point-1] - value\n",
    "    return search_list[ins_point-1] - value\n",
    "\n",
    "# a negative value means that new value is greater or , a positive value means that new value is lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_list = [1,2,3,4,5]\n",
    "value = 3\n",
    "print(bs.bisect_right([1,2,3,4,5],3))\n",
    "print(find_shortest_distance(search_list,value))\n",
    "print(find_shortest_distance_withdir(search_list,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_syllable_count(phrase):\n",
    "    vowels = {'a','e','i','o','u'}\n",
    "    consonants = {'b','c','d','f','g','h','j','k','l','m','n','p','q','r','s','t','v','w','x','z'}\n",
    "    y = {'y'}\n",
    "    length = len(phrase)\n",
    "    count_s = 0\n",
    "    # syllables are counted in middle and end from the starting consonant or y sound with vowel sound following\n",
    "    # in the starting: vowel sounds from a,e,i,o,u,and y are counted as 1 syllable regardless\n",
    "    # in the end: consonant - vowel end with e is not counted, every other case including y as the vowel is counted\n",
    "    first = phrase[0]\n",
    "    #print(first)\n",
    "    # dividing middle portion of word into pairs\n",
    "    pairs = [phrase[i:i+2] for i in range(len(phrase)-2)]\n",
    "    # getting ending pair\n",
    "    end = phrase[len(phrase)-2:len(phrase)]\n",
    "    \n",
    "    if first in vowels|y:\n",
    "        count_s = count_s + 1\n",
    "        #print(first,count_s)\n",
    "        \n",
    "    for p in pairs:\n",
    "        if p[0] in consonants|y and p[1] in vowels|y:\n",
    "            count_s = count_s + 1\n",
    "        #print(p,count_s)\n",
    "    #print(end)\n",
    "    if end[0] in consonants|y and end[1] in {'a','i','o','u','y'}:\n",
    "        count_s = count_s + 1\n",
    "        #print(end,count_s)\n",
    "    return count_s\n",
    "#     'employee'\n",
    "#     'e'   :1\n",
    "#     'em'  :0\n",
    "#     'mp'  :0\n",
    "#     'pl'  :0\n",
    "#     'lo'  :1\n",
    "#     'oy'  :0\n",
    "#     'ye'  :1\n",
    "#     'ee'  :0\n",
    "    # getting first letter\n",
    "\n",
    "def syllable_count(phrase):\n",
    "    toks = nltk.word_tokenize(phrase)\n",
    "    count = 0\n",
    "    for t in toks:\n",
    "        #syll_list = list(chain.from_iterable(cmud.get(t,[[0]])))\n",
    "        syll_list = cmud.get(t,[[0]])[0] # randomly choosing the first pronunciation\n",
    "        #print(syll_list)\n",
    "        if syll_list==[0]:\n",
    "            count = count + manual_syllable_count(t)\n",
    "        else:\n",
    "            count = count + sum([1 for y in syll_list if y[-1].isdigit()])\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Concept1 = [[np1]*len(npnp_bondstrengthdir[np1]) for np1 in npnp_bondstrengthdir.keys()]\n",
    "Concept1 = list(chain.from_iterable(Concept1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Concept2 = [np2 for np1 in npnp_bondstrengthdir.keys() for np2 in npnp_bondstrengthdir[np1].keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bondstrength = [npnp_bondstrengthdir[Concept1[i]][Concept2[i]] for i in range(len(Concept1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sentences in which concept occurs\n",
    "FA = [len(np_to_sent[np1]) for np1 in Concept1]\n",
    "FB = [len(np_to_sent[np2]) for np2 in Concept2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std deviation of occurence of concept: the spread - does it occur all over the document or just in one section. \n",
    "SdevA = [num.std(np_to_sent[np1]) for np1 in Concept1]\n",
    "SdevB = [num.std(np_to_sent[np2]) for np2 in Concept2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computing the mean bond strength of A to other concepts\n",
    "meanBSA = [num.mean(list(npnp_bondstrengthdir[np1].values())) for np1 in Concept1]\n",
    "meanBSB = [num.mean(list(npnp_bondstrengthdir.get(np2,{}).values())) for np2 in Concept2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computing average shortest distance of each A to a B and vice versa. metric for co-occurence\n",
    "OcA = [np_to_sent[np1] for np1 in Concept1]\n",
    "OcB = [np_to_sent[np2] for np2 in Concept2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAB=[]\n",
    "dBA=[]\n",
    "\n",
    "for i in range(len(Concept1)):\n",
    "    dAB.append(num.mean([abs(find_shortest_distance(OcB[i],o)) for o in OcA[i]]))\n",
    "    dBA.append(num.mean([abs(find_shortest_distance(OcA[i],o)) for o in OcB[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing number of mappings for Concept1, Concept2 respectively and how many of those concepts intersect') \n",
    "npnp_bondstrengthdir.get('complete exoneration',{})\n",
    "%time Amap = [len(npnp_bondstrengthdir[np1]) for np1 in Concept1]\n",
    "%time Bmap = [len(npnp_bondstrengthdir.get(np2,{})) for np2 in Concept2]\n",
    "%time AmapintersectBmap = [len(set(npnp_bondstrengthdir[Concept1[i]].keys()) & set(npnp_bondstrengthdir.get(Concept2[i],{}).keys())) for i in range(len(Concept1))]\n",
    "%time AminusB = [Amap[i]-AmapintersectBmap[i] for i in range(len(Concept1))]\n",
    "%time BminusA = [Bmap[i]-AmapintersectBmap[i] for i in range(len(Concept1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Edit word distance between the two concepts\n",
    "nptoWtkeys = list(np_to_sent.keys())\n",
    "print('word tokenizing each concept')\n",
    "%time nptoWtvals = [nltk.word_tokenize(np) for np in nptoWtkeys]\n",
    "nptoWt = dict(zip(nptoWtkeys,nptoWtvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('syllable counts for each concept')\n",
    "%time scvals = [syllable_count(np) for np in nptoWtkeys]\n",
    "nptoSC = dict(zip(nptoWtkeys,scvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtA = [nptoWt[np1] for np1 in Concept1]\n",
    "wtB = [nptoWt[np2] for np2 in Concept2]\n",
    "lenwtA = [len(wtA[i]) for i in range(len(Concept1))]\n",
    "lenwtB = [len(wtB[i]) for i in range(len(Concept1))]\n",
    "\n",
    "print('calculating word edit distance for concept1 and 2')\n",
    "%time editDwAtoB = [nltk.edit_distance(wtA[i],wtB[i])/lenwtA[i] for i in range(len(Concept1))]\n",
    "%time editDwBtoA = [nltk.edit_distance(wtA[i],wtB[i])/lenwtB[i] for i in range(len(Concept1))]\n",
    "\n",
    "print('calculating letter edit distance for concept1 and 2')\n",
    "lenA = [len(np1) for np1 in Concept1]\n",
    "lenB = [len(np2) for np2 in Concept2]\n",
    "%time editD =[nltk.edit_distance(Concept1[i],Concept2[i]) for i in range(len(Concept1))]\n",
    "editDlAtoB = [editD[i]/lenA[i] for i in range(len(Concept1))]\n",
    "editDlBtoA = [editD[i]/lenB[i] for i in range(len(Concept1))]\n",
    "\n",
    "## Jaccard word distance A to B\n",
    "print('calculating Jaccard distances by word')\n",
    "%time Jaccardw = [nltk.jaccard_distance(set(wtA[i]),set(wtB[i])) for i in range(len(Concept1))]\n",
    "print('calculating jaccard distance by letter')\n",
    "%time Jaccardl = [nltk.jaccard_distance(set(Concept1[i]),set(Concept2[i])) for i in range(len(Concept1))]\n",
    "\n",
    "lensents = len(sents)\n",
    "lennp = len(np_to_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AfirstOc = [np_to_sent[np1][0]/lensents for np1 in Concept1]\n",
    "BfirstOc = [np_to_sent[np2][0]/lensents for np2 in Concept2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllcountA = [nptoSC[np1] for np1 in Concept1]\n",
    "syllcountB = [nptoSC[np2] for np2 in Concept2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('getting reuters idf value for each concept')\n",
    "nptoReutersIDFvals = [num.mean([reuters_idf(t) for t in nptoWt[np1]]) for np1 in nptoWt.keys()]\n",
    "nptoReutersIDF = dict(zip(nptoWtkeys,nptoReutersIDFvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReutersIDFA = [nptoReutersIDF[np1] for np1 in Concept1]\n",
    "ReutersIDFB = [nptoReutersIDF[np2] for np2 in Concept2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('making into dataframe')\n",
    "\n",
    "%time df = pd.DataFrame({'Concept1':Concept1,'Concept2': Concept2,'FA':FA,'FB':FB,'SdevA':SdevA,'SdevB':SdevB, 'meanBSA':meanBSA, 'meanBSB':meanBSB,'dAB':dAB,'dBA':dBA,'Amap':Amap,'Bmap':Bmap,'AmapintersectBmap':AmapintersectBmap, 'AminusB':AminusB, 'BminusA':BminusA,'lenwtA':lenwtA,'lenwtB':lenwtB,'editDwAtoB':editDwAtoB, 'editDwBtoA':editDwBtoA, 'lenA':lenA,'lenB':lenB,'editDlAtoB':editDlAtoB, 'editDlBtoA':editDlBtoA, 'Jaccardw':Jaccardw, 'Jaccardl':Jaccardl,'AfirstOc':AfirstOc,'BfirstOc':BfirstOc,'syllcountA':syllcountA,'syllcountB':syllcountB,'ReutersIDFA':ReutersIDFA,'ReutersIDFB':ReutersIDFB, 'lennp':[lennp]*len(Concept1),'lensents':[lensents]*len(Concept1)})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'complete exoneration' in Concept2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['AfirstOc','BfirstOc'],ascending=True)[['Concept1','Concept2','AfirstOc','BfirstOc']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. graph is a dictionary\n",
    "2. decide whether to convert to dataframe?\n",
    "3. write function to return graph edges, and their corresponding sentences - these would not be in the npnp_bondstrength graph - nothing directs to them\n",
    "4. optional write function to look up on wiki or wordnet and add to the graph dynamically if edges are not understood.\n",
    "5. write function to slice out graphs with quantiles of number of nodes/ strength of bonds connectedness. add visualization. \n",
    "6. write function to take in a concept to understand and find the shortest path to it.  \n",
    "7. write function to return the shortest path to cover all the concepts in the sliced out graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning a direction to every mapping: \n",
    "\n",
    "Before we make the concept graph, we need to ensure the graph is directed and acyclic so it is clear which concept is the prerequisite and which is the more advanced topic. \n",
    "\n",
    "For now, the rules used to do this are pretty random.\n",
    "\n",
    "'Concept1', 'Concept2', 'FA', 'FB', 'SdevA', 'SdevB', 'meanBSA',\n",
    "       'meanBSB', 'dAB', 'dBA', 'Amap', 'Bmap', 'AmapintersectBmap', 'AminusB',\n",
    "       'BminusA', 'lenwtA', 'lenwtB', 'editDwAtoB', 'editDwBtoA', 'lenA',\n",
    "       'lenB', 'editDlAtoB', 'editDlBtoA', 'Jaccardw', 'Jaccardl', 'AfirstOc',\n",
    "       'BfirstOc', 'syllcountA', 'syllcountB', 'ReutersIDFA', 'ReutersIDFB',\n",
    "       'lennp', 'lensents']\n",
    "\n",
    "Hypotheses:\n",
    "1. lower reuters IDF value is a prerequisite concept.\n",
    "2. if there is an editDWA value less than one, then the concept with the lower lenwt is the prerequisite. For example:  \n",
    "Concept1                    statistical science\n",
    "Concept2             statistical methods mining\n",
    "lenwtA                                        2\n",
    "lenwtB                                        3\n",
    "editDwAtoB                                    1\n",
    "editDwBtoA                             0.666667\n",
    "ReutersIDFA                             9.28619\n",
    "ReutersIDFB                             7.45517\n",
    "It should be noted here, however, that the reuters IDF value for the second concept is lower. So we need to weight these hypotheses. \n",
    "3. AminusB, B minus A : the one with the higher value is the prerequisite.\n",
    "(this assumes that the prereq is mentioned several times, and is in fact more __central__. \n",
    "4. lower syllable count is prerequisite- this becomes important when both are one word concepts. \n",
    "5. first occurence in document is prerequisite\n",
    "\n",
    "For now, there is equal weightage to each of these rules and direction will be decided by majority.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cond1'] = df['ReutersIDFA']< df['ReutersIDFB']\n",
    "df['Cond2'] =  df['lenwtA']<df['lenwtB']\n",
    "#df['Cond2'] = df['meanBSA'] < df['meanBSB']\n",
    "df['Cond3'] = df['AminusB'] > df['BminusA'] # A maps to more concepts that don't map to B within this document. \n",
    "df['Cond4'] = df['syllcountA'] < df['syllcountB']\n",
    "df['Cond5'] = df['AfirstOc'] < df['BfirstOc']\n",
    "df['Cond6'] = df['SdevA'] > df['SdevB'] # spread of A in document is more than spread of B\n",
    "df['Cond7'] = df['FA']>df['FB']\n",
    "#df['Cond8'] = df['editDlAtoB'] < 1\n",
    "#df['Cond9'] = df['Jaccardw'] >= 0.5\n",
    "#df['Cond10'] = df['Jaccardl'] > 0.5\n",
    "\n",
    "#df['Direction'] = num.sum(df.loc[:,'Cond1':'Cond7'],axis=1)\n",
    "df['Direction'] = df['Cond3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing to csv file for easier exploration temporarily\n",
    "df.to_csv('df_CLT.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdir = df[df['Direction']>=1]\n",
    "print(len(dfdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "%time G = nx.from_pandas_edgelist(dfdir,'Concept1','Concept2', create_using=nx.DiGraph())\n",
    "#%time num_cycles = len(list(nx.simple_cycles(G)))\n",
    "#print('Number of cycles in this graph = '+str(num_cycles))\n",
    "#print(G)\n",
    "#print(nx.draw(G))\n",
    "Conceptdata.sort_values(by=['Frequency','Sdev'],ascending = [0,0]).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodelist = list(Conceptdata.sort_values(by=['Frequency','Sdev'],ascending = [0,0]).head(25)['Concept'])\n",
    "\n",
    "# checkign if graph has cycles\n",
    "%time loadcentdict = nx.load_centrality(G)\n",
    "%time dfdirConcept1 = list(dfdir['Concept1'])\n",
    "%time dfdirConcept2 = list(dfdir['Concept2'])\n",
    "%time centralityA = [loadcentdict.get(np1,0) for np1 in dfdirConcept1]\n",
    "%time centralityB = [loadcentdict.get(np2,0) for np2 in dfdirConcept2]\n",
    "\n",
    "dfdir['centralityA'] = centralityA\n",
    "dfdir['centralityB'] = centralityB\n",
    "\n",
    "%time dfdir.sort_values(by = ['centralityA'],ascending=[False]).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(dfdir['centralityA'],dfdir['meanBSA'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find paths from each of these to the others. \n",
    "%time paths=dict(nx.all_pairs_shortest_path(G,cutoff=None))\n",
    "type(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_allpairs(concept_list):\n",
    "    nodelist = [paths.get(cl1, {}).get(cl2,None) for cl1 in concept_list for cl2 in concept_list if paths.get(cl1, {}).get(cl2,None) is not None]\n",
    "    nodelist = list(chain.from_iterable(nodelist))\n",
    "    return list(set(nodelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_concept_list = list(Conceptdata.sort_values(by=['Frequency','Sdev'],ascending = [0,0]).head(5)['Concept'])\n",
    "#start_concept_list = ['trump','midterm election']\n",
    "print(start_concept_list)\n",
    "# now get the paths from all pairs in the concept list and the corresponding nodes \n",
    "nodelist = get_nodes_allpairs(start_concept_list)\n",
    "print(nodelist)\n",
    "start_concept_edges = dfdir[dfdir['Concept1'].isin(nodelist) & dfdir['Concept2'].isin(nodelist)]\n",
    "start_concept_from = set(start_concept_edges['Concept1'])\n",
    "#print(start_concept_edges)\n",
    "print(start_concept_from)\n",
    "start_concept_to = set(start_concept_edges['Concept2'])\n",
    "print(start_concept_to)\n",
    "len(start_concept_edges)\n",
    "plt.figure(figsize=(20,10))\n",
    "nx.draw_circular(G.subgraph(list(start_concept_from|start_concept_to)),with_labels=True, font_size=18,node_size=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np_to_sent['trump'])\n",
    "print(np_to_sent['president'])\n",
    "print(paths['midterm election']['trump'])\n",
    "#[p for p in paths['learners'].keys() if p in nodelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_indices(np1,np2,max_distance=3):\n",
    "    sents1 = np_to_sent[np1]\n",
    "    sents2 = np_to_sent[np2]\n",
    "    ind1 = 0\n",
    "    ind2 = 0\n",
    "    tuplist = []\n",
    "    lensents1 = len(sents1)\n",
    "    print(lensents1)\n",
    "    lensents2 = len(sents2)\n",
    "    print(lensents2)\n",
    "    while(ind1<lensents1 and ind2 <lensents2):\n",
    "        #print(ind1,ind2)\n",
    "        if (sents1[ind1]<sents2[ind2]):\n",
    "            #print('sent1 less than sent2')\n",
    "            if sents2[ind2]-sents1[ind1]<=max_distance:\n",
    "                tuplist.append((sents1[ind1],sents2[ind2]))\n",
    "                ind1 = ind1+1\n",
    "                ind2 = ind2 + 1\n",
    "            else:\n",
    "                #ind1 = bs.bisect_left(sents1,sents2[ind2])\n",
    "                ind1 = ind1 + 1\n",
    "        elif (sents1[ind1]>sents2[ind2]):\n",
    "            #print('sent2 less than sent1')\n",
    "            if sents1[ind1]-sents2[ind2] <= max_distance:\n",
    "                tuplist.append((sents2[ind2],sents1[ind1]))\n",
    "                ind1 = ind1 + 1\n",
    "                ind2 = ind2 + 1\n",
    "            else:\n",
    "                #ind2 = bs.bisect_left(sents2,sents1[ind1])\n",
    "                ind2 = ind2 + 1\n",
    "        else:\n",
    "            tuplist.append((sents1[ind1],sents2[ind2]))\n",
    "            ind1 = ind1+1\n",
    "            ind2 = ind2+1\n",
    "    return tuplist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blurbs(np1,np2,max_distance=3):\n",
    "    blurblist = []\n",
    "    tuplist = get_sentence_indices(np1,np2,max_distance)\n",
    "    print(tuplist)\n",
    "    for t in tuplist:\n",
    "        blurb = []\n",
    "        print(t)\n",
    "        blurb = ' '.join(sents[t[0]:t[1]+1]).replace('\\n', ' ').replace('\\r', '')\n",
    "        print(blurb)\n",
    "        blurblist.append(blurb)\n",
    "    return blurblist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurblist = get_blurbs('fitness function','rnns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_to_sent['rnns']\n",
    "print(sents[148])\n",
    "print(sents[148:149])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blurblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find paths from each of these nodes to others.\n",
    "# indegreecentralitydict = nx.in_degree_centrality(G)\n",
    "# outdegreecentralitydict = nx.out_degree_centrality(G)\n",
    "\n",
    "# # in_centralityA = [indegreecentralitydict.get(np1,0) for np1 in Concept1]\n",
    "# # in_centralityB = [indegreecentralitydict[np2] for np2 in Concept2]\n",
    "# # out_centralityA = [outdegreecentralitydict[np1] for np1 in Concept1]\n",
    "# # out_centralityB = [outdegreecentralitydict[np1] for np2 in Concept2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to find common paras - this is possibly a redo of npnp_bondstrengthdir, maybe club this in there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_close_text(np1,np2,max_sent_dist=3):\n",
    "    sentlistA = np_to_sent.get(np1,[0])\n",
    "    sentlistB = np_to_sent.get(np2,[0])\n",
    "    global lensents\n",
    "    \n",
    "    SA_dist = [(sA,find_shortest_distance_withdir(sentlistB,sA)) for sA in sentlistA if find_shortest_distance(sentlistB,sA)<=max_sent_dist]\n",
    "    index_tuples = [(tup[0],tup[0]-tup[1]) for tup in SA_dist]\n",
    "    return index_tuples\n",
    "# maybe pass a list of ranges? - convert each tuple to range, and sets, and if any intersections exist, then make them one. \n",
    "# or back calculate from npnp_bondstrengthdir?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take dAB and dBA, make blurbs indices of sentences +- rounded up int dAB.\n",
    "# merge blurb indices with max(dAB,dBA) and return (this might be recursive)\n",
    "\n",
    "def display_sentences(np1,np2):\n",
    "\n",
    "    print(index_tup)\n",
    "    if batch_size>len(index_tup):\n",
    "        batch_size = len(index_tup)\n",
    "    for i in range(batch_size):\n",
    "        textblob=''\n",
    "        for s in sents[index_tup[i][0]:index_tup[i][1]+1]:\n",
    "            textblob = textblob + ' '+s.replace('\\n', ' ').replace('\\r', '')\n",
    "        print(textblob)\n",
    "\n",
    "# print the concepts in a different color. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_close_text('trump','mueller',4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take input on what concepts are not known:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to calculate idf for different corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo 3/25/2019\n",
    "Add the following columns to df, and increase speed\n",
    "2. stem/lem the concepts word by word and unite them with the first mention and rerun analysis:\n",
    " - how to get simple adjective from comparative and superlative?\n",
    "3. annotate: semantic notes and roles for each concept. identify prerequisites and make direction column manually.\n",
    "4. rough rule-based or n-gram/ sentence structure based role classification from pos and keywords in sentence?\n",
    "5. centrality from graph library\n",
    "6. annotate for training and test data using 5 more wiki articles atleast: also save total number of concepts and sentences, etc. i.e columns for the data set\n",
    "7. setup rough plan for classification - what algorithms are you going to try. do some exploration. \n",
    "8. google ngrams or scientific corpus IDF \n",
    "9. modify Reuters IDF to take in word tokenized noun phrase and consider each one separately, return the average. \n",
    "10. what other pos taggers are available, and should we look at those instead. should we clean up our sents a little more?\n",
    "11. consider stemming for idf values and for combining singular/ plural forms - going to take the shortest word that is in the cmud dictionary. \n",
    "12. what does sorting by descending amapintersectbmap show? does text1.similar show anything interesting?\n",
    "13. while displaying sentences with a concept, it should not include noun phrases containing more than just that concept? like ___ dance and just dance - only select sentences that display the concept asked for. \n",
    "14.  one way of measuring the clt is to get the combined idf for all the concepts, the number of total concepts and number of unknown concepts. Then every sentence/blurb will have a clt number, then you can select blurbs so that clt is always minimized. \n",
    "15. while calculating combined reuters_idf: do not include stop words. (and, of etc.)\n",
    "\n",
    "Todo 3/28/2019\n",
    "interactive graph visualization\n",
    "\n",
    "1. How do we have nodes with no parents or children?? does it automatically show the concepts in concept data that are not pointed to?\n",
    "\n",
    "Sentence classification with rules and annotation. (the J tree graph structure as input for ML maybe?)\n",
    "\n",
    "Todo 3/29/2019\n",
    "simple form GUI in tkinter\n",
    "\n",
    "Todo 3/30/2019\n",
    "inference engine: validation, inference, user friendly form questions. review question formats.\n",
    "\n",
    "I came across the same problem, searched the web with no answer, then discovered that it actually can be done with the WordNet lemmatizer in nltk.\n",
    "\n",
    "Recall that WordNet has those simplified pos tags:\n",
    "\n",
    "n    NOUN \n",
    "v    VERB \n",
    "a    ADJECTIVE \n",
    "s    ADJECTIVE SATELLITE \n",
    "r    ADVERB \n",
    "among which the adjective tags, aand s, can be used for the normalization.\n",
    "\n",
    ">>> from nltk.stem.wordnet import WordNetLemmatizer\n",
    ">>> wnl = WordNetLemmatizer()\n",
    ">>> wnl.lemmatize('biggest', 'a')\n",
    "u'big'\n",
    ">>> wnl.lemmatize('better', 'a')\n",
    "u'good'\n",
    "Here the second parameter does the magic trick. If left blank, it defaults to 'n', or wordnet.NOUN inlemmatize(). Similarly, it should be put explicitly as 'v' or 'r' for normalizing verbs and adverbs, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl.lemmatize('pickling','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pattern3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern3.en import conjugate, lemma, lexeme, parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_idf('of')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce tokens to root so that variables and variable can be counted as one np\n",
    "nodelist = []\n",
    "for i in np_to_sent['president']:\n",
    "    nodelist.extend(np for np in sent_to_np[i])\n",
    "nodelist = set(list(chain.from_iterable(nodelist)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
