{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept extraction from text\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading text file into string \n",
    "\n",
    "### Option 1. Downloading a wikipedia article's text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of material\n",
      "40417\n",
      "Sample of text\n",
      "\n",
      "The human digestive system consists of the gastrointestinal tract plus the accessory organs of digestion (the tongue, salivary glands, pancreas, liver, and gallbladder). Digestion involves the breakdown of food into smaller and smaller components, until they can be absorbed and assimilated into the body. The process of digestion has three stages. The first stage is the cephalic phase of digestion which begins with gastric secretions in response to the sight and smell of food. This stage include\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'https://en.wikipedia.org/wiki/Human_digestive_system'\n",
    "\n",
    "source = requests.get(url).text\n",
    "soup = BeautifulSoup(source,'lxml')\n",
    "\n",
    "\n",
    "text_set = soup.find_all(['p']) ## This will skip headings ('h2','h3') and lists that are made as links( 'li')\n",
    "text_list = [p1.get_text() for p1 in text_set]\n",
    "tags_list = [p1.name for p1 in text_set ]\n",
    "\n",
    "rawtxt = ''.join(text_list)\n",
    "\n",
    "print(\"length of material\")\n",
    "print(len(rawtxt))\n",
    "\n",
    "print(\"Sample of text\")\n",
    "print(rawtxt[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save rawtxt as is for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'A Mind For Numbers_ How to Excel at Math and Science (Even If You Flunked Algebra).txt'\n",
    "#path_name = \"C:/Users/Arati/Documents/personal docs/cognitive science and education/\"\n",
    "# path_name = \"C:/Users/Arati/Documents/personal docs/python_introduction_course/textdata/\"\n",
    "# path_name = \"C:/Users/Arati/Documents/personal docs/cognitive science and education/\"\n",
    "#filename = 'Physics  for IIT- JEE (Mains & Advanced)  Vol. 1 of 4_  Complete Study Pack of Physics_ Mechanics  for Engineering Entrance Examination (Sachan Book 62).txt'\n",
    "path_name = 'C:/Users/Arati/Conceptmap project/IntelligentTutoringSystem_Experiments/processed_data'\n",
    "filename = 'Wiki_human_digestive_system'\n",
    "with open(path_name + filename,\"a\",encoding=\"utf-8\") as myfile:\n",
    "    myfile.write(rawtxt)\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. Getting file from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex PTSD_ From Surviving to Thriving_ A GUIDE AND MAP FOR RECOVERING FROM CHILDHOOD TRAUMA.txt\n",
      "563960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "563960"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = ['Complex PTSD_ From Surviving to Thriving_ A GUIDE AND MAP FOR RECOVERING FROM CHILDHOOD TRAUMA.txt']\n",
    "# path_name = \"C:/Users/Arati/Documents/personal docs/python_introduction_course/textdata/\"\n",
    "path_name = \"C:/Users/Arati/Conceptmap project/IntelligentTutoringSystem_Experiments/raw_data/\"\n",
    "rawtxt = []\n",
    "for f in fnames:\n",
    "    with open(path_name + f, \"r\",encoding = \"utf-8\") as myfile:\n",
    "        rawtxt.append(myfile.read())\n",
    "    print(f)\n",
    "for t in rawtxt:\n",
    "    print(len(t))\n",
    "rawtxt = ('\\n').join(rawtxt)\n",
    "len(rawtxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'wikihow_1'\n",
    "path_name = \"C:/Users/Arati/Documents/personal docs/python_introduction_course/textdata/\"\n",
    "with open (path_name +filename+'.txt', \"r\",encoding=\"utf-8\") as myfile:\n",
    "    rawtxt0=myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'wikipedia_4'\n",
    "path_name = \"C:/Users/Arati/Documents/personal docs/python_introduction_course/textdata/\"\n",
    "with open (path_name +filename+'.txt', \"r\",encoding=\"utf-8\") as myfile:\n",
    "    rawtxt1=myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawtxt = rawtxt0+rawtxt1\n",
    "\n",
    "#rawtxt = rawtxt.encode('ascii','ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawtxt = 'Classical mechanics describes the motion of macroscopic objects, from projectiles to parts of machinery, and astronomical objects, such as spacecraft, planets, stars and galaxies.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a not a key\n",
      "b not a key\n",
      "c not a key\n",
      "d not a key\n",
      "e not a key\n",
      "f not a key\n",
      "g not a key\n",
      "h not a key\n",
      "i not a key\n",
      "j not a key\n",
      "k not a key\n",
      "l not a key\n",
      "m not a key\n",
      "n not a key\n",
      "o not a key\n",
      "p not a key\n",
      "q not a key\n",
      "r not a key\n",
      "s not a key\n",
      "t not a key\n",
      "u not a key\n",
      "v not a key\n",
      "w not a key\n",
      "x not a key\n",
      "y not a key\n",
      "z not a key\n"
     ]
    }
   ],
   "source": [
    "for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "    try:\n",
    "        print(wnl.lemmatize('am',letter))\n",
    "    except:\n",
    "        print(letter+' not a key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting list of concepts:\n",
    "\n",
    "### Importing libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.chunk import *\n",
    "from nltk.chunk.util import *\n",
    "from nltk.chunk.regexp import *\n",
    "from nltk import Tree\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "import nltk\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('../processed_data/stopconcept_list.txt', \"r\",encoding=\"utf-8\") as myfile:\n",
    "    stop_concepts=myfile.read()\n",
    "stop_concepts = stop_concepts.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_concepts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting\n",
    "#### Option 1: split on periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = rawtxt.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Pretrained NLTK sentence splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.sent_tokenize(rawtxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Train unsupervised on domain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(rawtxt)\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = tokenizer.tokenize(rawtxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in text 3330\n",
      "3330\n",
      "Sample of sentences:\n",
      "['Title : Complex PTSD: From Surviving to Thriving: A GUIDE AND MAP FOR RECOVERING FROM CHILDHOOD TRAUMA\\nAuthor: Walker, Pete\\n\\nGenerate From: eCore v0.9.12.751 [ http://www.epubor.com/ecore.html ]\\n\\nTechnology Website: http://www.epubor.com\\n\\n\\n\\n [image file=image8.jpg]  [image file=image17.jpg] Testimonials about Pete Walker’s first book, The Tao of Fully Feeling, and his website: www.pete-walker.comI am writing from Survivors of Abuse Recovering (S.O.A.R.) Society, located in Canada.', 'We would like to include “13 Steps for Managing Flashbacks” in our resource manual.I found myself.', 'I found myself in your words.', 'It’s as if you had unzipped me, stepped inside my traumatized inner self, meandered around a bit, come back outside, and wrote about what you discovered inside of me.', 'For the first time in my life.......and I’m in my fifties now........I don’t feel defective......or crazy.......or “weird”.......or even unlovable.', '— D.M.I sat in the San Francisco Airport reading your book (in the washroom, shaking and weeping) to get the courage to go the next leg of the trip.', 'It helped me so much just to know that you live in that area-strange when I haven’t even met you!', 'Your website and book are invaluable to me.', '— A. R.I want to thank you so much for all the help you have given me (and all the people I’ve passed your website link onto since finding out about it).', 'Your understanding of emotional flashbacks has made an enormous difference in my life.', 'I’ve gone from being smashed about by huge waves to having a surfboard on which I can ride at least some of them, and even if I fall off into it, I know it won’t last forever.', '— J, New ZealandThank you for all of your educational information with regards to PTSD and abandonment.', 'I have finally found something that I have tried to explain to therapists for years.', 'Every single piece of information is exactly what I experience from my PTSD and attachment depression.', '— AI thank you on a personal and professional level.', 'Your articles on healing from CPTSD have excited me and validated me both.', 'I will be a better therapist now, and heal further in my own life.', '— DYour article will be one of my regular handouts now to my clients.', 'Needless to say I feel this information and the way you articulate it is a life saver!', '— L.PHow impactful all you have written has been for me and how much healing I have found in the pages of your website.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences in text \"+str(len(sents)))\n",
    "print(len(sents))\n",
    "\n",
    "print(\"Sample of sentences:\")\n",
    "print(sents[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token handling functions: \n",
    "1. validchar(wrd): checks if token is a valid alphanumeric+hyphens word\n",
    "2. lemmatize_by_pos(tag) lemmatizes token by part of speech\n",
    "3. chunk_this(grammar_rule_key,sentence_tags) chunks a particular grammar rule key (see chunkrules)\n",
    "4. eqn_label: extracts equation terms and replaces all occurences in text with a textkey, which is then treated as a noun phrase. Also updates equation dictionary\n",
    "5. display_equation (displays equation term by key)\n",
    "6. chunker: chunks each sentence by each chunking rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validchar(wrd):\n",
    "    p = re.compile(r'[^0-9a-zA-Z_-]')\n",
    "    if p.search(wrd) is None:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "# wdlist = {'be','can','could','should','as','would','must','ought','will','i',\"won't\",\"can't\",\"isn't\",\"shan't\",'to','the','a','an',\n",
    "#           'of','at','by','if','use', 'shall','about','around','and','or','try','he','she','this','they','them','these','those',\n",
    "#           'it','for','useful','not',\"it's\",'its','with','which','that','than'}\n",
    "wdlist = {}\n",
    "def lemmatize_by_pos(tag,wdlist):\n",
    "    token = tag[0].lower()\n",
    "    pos = tag[1]\n",
    "#     if token in stop_words:\n",
    "#         return (token,pos)\n",
    "    if pos.startswith('J'):\n",
    "        # adjective form\n",
    "        lemma = wnl.lemmatize(token,'s')\n",
    "    elif pos.startswith('N'):\n",
    "        # noun form\n",
    "        lemma = wnl.lemmatize(token,'n')\n",
    "    elif pos.startswith('R'):\n",
    "        # adverb\n",
    "        lemma = wnl.lemmatize(token,'r')\n",
    "    elif pos.startswith('V'):\n",
    "        lemma = wnl.lemmatize(token,'v')\n",
    "    else:\n",
    "        lemma = token\n",
    "    if lemma in wdlist:\n",
    "        pos = lemma\n",
    "    return (lemma,pos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "global eqn_dict\n",
    "eqn_dict = {}\n",
    "global eqn_count\n",
    "eqn_count = 1\n",
    "\n",
    "def eqn_label(tokens):\n",
    "    global eqn_count\n",
    "    global eqn_dict\n",
    "    EQNlist = [wrd for wrd in tokens if not(wrd.isalnum()) and re.search(r'[\\[\\]\\{\\}\\+*^=_%$]',wrd) and len(wrd)>1 ]\n",
    "    ## replace queations with a label and save to equation dictionary\n",
    "    for eqn in EQNlist:\n",
    "        \n",
    "        if not(eqn in eqn_dict):\n",
    "            \n",
    "            eqn_dict[eqn] = ''.join(['equation',str(eqn_count)])\n",
    "            eqn_count = eqn_count + 1                          \n",
    "        else:    \n",
    "            tokens[tokens.index(eqn)] = eqn_dict[eqn]\n",
    "                  \n",
    "    return tokens\n",
    "\n",
    "global inv_eqn_dict\n",
    "inv_eqn_dict = dict([[value,key] for key,value in eqn_dict.items()])\n",
    "\n",
    "def display_equation(reptokens):\n",
    "    for wrd in reptokens:\n",
    "        if wrd in inv_eqn_dict:\n",
    "            reptokens[reptokens.index(wrd)] = inv_eqn_dict[wrd]\n",
    "    return reptokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up chunking rules:\n",
    "\n",
    "Chunking done in batches to enable overlapping tokens to be extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkrules = {}\n",
    "\n",
    "# Define chunking rules here:\n",
    "chunkrules['JJNP'] = r\"\"\"    \n",
    "    JJNP: {<RB.*>?<J.*>?<NN.*>{1,}}       \n",
    "\"\"\"\n",
    "## Examples: \"reusable contactless stored value smart card\"\n",
    "\n",
    "def chunk_this(grammar_rule_key,sentence_tags):\n",
    "    setlist = []\n",
    "    cp = nltk.RegexpParser(chunkrules[grammar_rule_key])\n",
    "    J = cp.parse(sentence_tags) \n",
    "    for i in range(len(J)):\n",
    "        if not(isinstance(J[i],tuple)):\n",
    "            if (J[i].label()==grammar_rule_key):\n",
    "                setlist.append((' '.join([J[i][j][0] for j in range(len(J[i])) if (validchar(J[i][j][0])==1)])))\n",
    "    setlist = list(set(setlist))\n",
    "    setlist = [wrd.lower() for wrd in setlist if (len(wrd)>1 and wrd not in stop_concepts)]\n",
    "    return setlist\n",
    "\n",
    "def chunker(sentence_tags):\n",
    "    return [chunk_this(key,sentence_tags)  for key in chunkrules]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "sent_to_np = {}\n",
    "sent_to_ltags = {}\n",
    "sent_to_tags = {}\n",
    "\n",
    "for i in range(len(sents)):\n",
    "    tokens = word_tokenize(sents[i])\n",
    "    reptokens = eqn_label(tokens)\n",
    "    tags = nltk.pos_tag(reptokens)\n",
    "    lemmatags = [lemmatize_by_pos(t,wdlist) for t in tags]\n",
    "    sent_to_np[i] = chunker(lemmatags)\n",
    "    sent_to_ltags[i] = lemmatags\n",
    "    sent_to_tags[i] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['definition',\n",
       "  'developmental task',\n",
       "  'pre-verbal child',\n",
       "  '2-3 year-olds',\n",
       "  'feeling']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_to_np[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten lists of lists containing chunks of different rules, dictionary of np to sent\n",
    "import itertools\n",
    "sent_to_npflat = {}\n",
    "np_to_sent = {}\n",
    "for key in sent_to_np:\n",
    "    sent_to_npflat[key] = list(set((itertools.chain(*sent_to_np[key]))))  \n",
    "    for np in sent_to_npflat[key]:            \n",
    "        if np in np_to_sent:                           \n",
    "            np_to_sent[np].append(key)\n",
    "        else:                \n",
    "            np_to_sent[np]=[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe with some metrics:\n",
    "- Concept: concept phrase\n",
    "- Occurence: list of sentences in which the phrase occurs\n",
    "- Frequency: number of sentences in which the phrase occurs\n",
    "- Mean: average of sentence numbers in the text in which the phrase occurs normalized to number of sentences\n",
    "- Median: median of sentence numbers in the text in which the phrase occurs normalized to number of sentences. Lets us know if phrase occurs much more in the beginning of the text, or towards the end. can indicate how central the phrase is to the text. \n",
    "- Sdev: standard deviation of the sentences in which the phrase occurs (indicates the dispersion of the phrase in the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as num\n",
    "import pandas as pd\n",
    "Concept = pd.Series([key for (key,value) in np_to_sent.items()])\n",
    "Occurence = pd.Series([num.array(value) for (key,value) in np_to_sent.items()])\n",
    "Frequency = pd.Series([len(o) for o in Occurence])\n",
    "Mean= pd.Series([num.mean(o) for o in Occurence])/len(sents)\n",
    "Median = pd.Series([num.median(o) for o in Occurence])/len(sents)\n",
    "Sdev = pd.Series([num.std(o) for o in Occurence])/len(sents)\n",
    "Conceptdata = pd.DataFrame({'Concept':Concept,'Occurence':Occurence,'Frequency':Frequency,'Mean':Mean,'Median':Median,'Sdev':Sdev})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Occurence</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Sdev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>parent</td>\n",
       "      <td>[85, 113, 134, 136, 144, 163, 213, 214, 217, 2...</td>\n",
       "      <td>178</td>\n",
       "      <td>0.409674</td>\n",
       "      <td>0.416066</td>\n",
       "      <td>0.268445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>child</td>\n",
       "      <td>[113, 116, 143, 147, 203, 204, 205, 206, 208, ...</td>\n",
       "      <td>172</td>\n",
       "      <td>0.366698</td>\n",
       "      <td>0.286937</td>\n",
       "      <td>0.260058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>critic</td>\n",
       "      <td>[87, 118, 434, 435, 436, 469, 513, 515, 554, 6...</td>\n",
       "      <td>141</td>\n",
       "      <td>0.572694</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.200868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>flashback</td>\n",
       "      <td>[1, 146, 150, 159, 161, 163, 173, 178, 191, 19...</td>\n",
       "      <td>139</td>\n",
       "      <td>0.477972</td>\n",
       "      <td>0.460661</td>\n",
       "      <td>0.242088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>survivor</td>\n",
       "      <td>[0, 95, 136, 147, 188, 191, 193, 199, 201, 295...</td>\n",
       "      <td>133</td>\n",
       "      <td>0.460261</td>\n",
       "      <td>0.457357</td>\n",
       "      <td>0.272234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>fear</td>\n",
       "      <td>[87, 97, 161, 173, 213, 214, 239, 250, 332, 34...</td>\n",
       "      <td>125</td>\n",
       "      <td>0.563133</td>\n",
       "      <td>0.686486</td>\n",
       "      <td>0.274024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>feeling</td>\n",
       "      <td>[30, 113, 172, 193, 201, 296, 355, 371, 385, 3...</td>\n",
       "      <td>104</td>\n",
       "      <td>0.529106</td>\n",
       "      <td>0.632883</td>\n",
       "      <td>0.304308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>client</td>\n",
       "      <td>[17, 101, 125, 389, 394, 519, 537, 587, 589, 5...</td>\n",
       "      <td>101</td>\n",
       "      <td>0.582511</td>\n",
       "      <td>0.703003</td>\n",
       "      <td>0.275223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>life</td>\n",
       "      <td>[4, 9, 30, 35, 38, 56, 66, 67, 149, 187, 188, ...</td>\n",
       "      <td>85</td>\n",
       "      <td>0.400512</td>\n",
       "      <td>0.413213</td>\n",
       "      <td>0.291423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>recovery</td>\n",
       "      <td>[87, 97, 320, 360, 504, 520, 536, 579, 593, 60...</td>\n",
       "      <td>79</td>\n",
       "      <td>0.443844</td>\n",
       "      <td>0.375976</td>\n",
       "      <td>0.274904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>childhood</td>\n",
       "      <td>[64, 118, 121, 185, 203, 302, 304, 313, 363, 4...</td>\n",
       "      <td>77</td>\n",
       "      <td>0.500581</td>\n",
       "      <td>0.461261</td>\n",
       "      <td>0.281293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>shame</td>\n",
       "      <td>[148, 159, 162, 173, 213, 406, 567, 569, 570, ...</td>\n",
       "      <td>73</td>\n",
       "      <td>0.582134</td>\n",
       "      <td>0.703003</td>\n",
       "      <td>0.272440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>cptsd</td>\n",
       "      <td>[15, 87, 126, 127, 134, 138, 139, 143, 145, 14...</td>\n",
       "      <td>72</td>\n",
       "      <td>0.320938</td>\n",
       "      <td>0.231682</td>\n",
       "      <td>0.293434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>anger</td>\n",
       "      <td>[70, 278, 308, 376, 379, 391, 428, 601, 836, 9...</td>\n",
       "      <td>66</td>\n",
       "      <td>0.582806</td>\n",
       "      <td>0.645946</td>\n",
       "      <td>0.235659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>therapist</td>\n",
       "      <td>[12, 22, 62, 135, 192, 193, 354, 467, 568, 578...</td>\n",
       "      <td>58</td>\n",
       "      <td>0.609729</td>\n",
       "      <td>0.820871</td>\n",
       "      <td>0.319280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>year</td>\n",
       "      <td>[12, 34, 39, 62, 75, 80, 100, 101, 104, 114, 1...</td>\n",
       "      <td>57</td>\n",
       "      <td>0.354270</td>\n",
       "      <td>0.273574</td>\n",
       "      <td>0.304698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>[11, 140, 141, 147, 173, 191, 202, 214, 334, 5...</td>\n",
       "      <td>54</td>\n",
       "      <td>0.472339</td>\n",
       "      <td>0.434835</td>\n",
       "      <td>0.295988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>work</td>\n",
       "      <td>[54, 56, 66, 101, 124, 256, 325, 326, 376, 412...</td>\n",
       "      <td>52</td>\n",
       "      <td>0.479435</td>\n",
       "      <td>0.465916</td>\n",
       "      <td>0.320579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>depression</td>\n",
       "      <td>[121, 148, 379, 767, 769, 774, 836, 871, 943, ...</td>\n",
       "      <td>51</td>\n",
       "      <td>0.645428</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.243217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>tear</td>\n",
       "      <td>[376, 394, 404, 456, 481, 504, 565, 567, 996, ...</td>\n",
       "      <td>49</td>\n",
       "      <td>0.540075</td>\n",
       "      <td>0.600601</td>\n",
       "      <td>0.246311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>danger</td>\n",
       "      <td>[220, 224, 336, 382, 448, 804, 889, 909, 949, ...</td>\n",
       "      <td>47</td>\n",
       "      <td>0.518516</td>\n",
       "      <td>0.493093</td>\n",
       "      <td>0.273745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>image equation4</td>\n",
       "      <td>[124, 138, 300, 354, 362, 401, 407, 435, 441, ...</td>\n",
       "      <td>47</td>\n",
       "      <td>0.507584</td>\n",
       "      <td>0.549550</td>\n",
       "      <td>0.296061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>emotional flashback</td>\n",
       "      <td>[9, 44, 60, 146, 147, 149, 151, 157, 162, 172,...</td>\n",
       "      <td>46</td>\n",
       "      <td>0.362795</td>\n",
       "      <td>0.366967</td>\n",
       "      <td>0.270388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>love</td>\n",
       "      <td>[85, 385, 402, 467, 494, 522, 543, 576, 606, 6...</td>\n",
       "      <td>45</td>\n",
       "      <td>0.540674</td>\n",
       "      <td>0.573574</td>\n",
       "      <td>0.318084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>friend</td>\n",
       "      <td>[97, 115, 127, 135, 281, 354, 390, 419, 467, 4...</td>\n",
       "      <td>45</td>\n",
       "      <td>0.506894</td>\n",
       "      <td>0.532733</td>\n",
       "      <td>0.315418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3704</th>\n",
       "      <td>new york</td>\n",
       "      <td>[1179, 3248, 3249, 3250, 3252, 3256, 3257, 326...</td>\n",
       "      <td>44</td>\n",
       "      <td>0.973778</td>\n",
       "      <td>0.988739</td>\n",
       "      <td>0.094771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>family</td>\n",
       "      <td>[85, 104, 115, 136, 142, 169, 172, 230, 231, 2...</td>\n",
       "      <td>43</td>\n",
       "      <td>0.319352</td>\n",
       "      <td>0.260661</td>\n",
       "      <td>0.284193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>inner critic</td>\n",
       "      <td>[406, 435, 500, 724, 765, 887, 944, 1185, 1445...</td>\n",
       "      <td>43</td>\n",
       "      <td>0.568434</td>\n",
       "      <td>0.580480</td>\n",
       "      <td>0.218128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>pain</td>\n",
       "      <td>[83, 85, 231, 233, 415, 425, 428, 429, 755, 78...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.501852</td>\n",
       "      <td>0.580030</td>\n",
       "      <td>0.271745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>ability</td>\n",
       "      <td>[308, 358, 377, 382, 387, 437, 478, 479, 482, ...</td>\n",
       "      <td>41</td>\n",
       "      <td>0.378034</td>\n",
       "      <td>0.260661</td>\n",
       "      <td>0.255495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>mistake</td>\n",
       "      <td>[331, 644, 1461, 1693, 1799, 2213, 2716, 2743,...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.585796</td>\n",
       "      <td>0.602402</td>\n",
       "      <td>0.268661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>hopelessness</td>\n",
       "      <td>[174, 1022, 1447, 1689, 1764, 2406, 2561, 2562...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.590571</td>\n",
       "      <td>0.626126</td>\n",
       "      <td>0.263755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>hurt</td>\n",
       "      <td>[601, 943, 1097, 1447, 2213, 2244, 2375, 2743,...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.598048</td>\n",
       "      <td>0.669219</td>\n",
       "      <td>0.261023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>fawn response</td>\n",
       "      <td>[227, 958, 1038, 1146, 1272, 1277, 1300, 1305,...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.337748</td>\n",
       "      <td>0.382733</td>\n",
       "      <td>0.097885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>victim</td>\n",
       "      <td>[207, 879, 1318, 1328, 1724, 1765, 2881, 2974,...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.577357</td>\n",
       "      <td>0.523874</td>\n",
       "      <td>0.292762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>helplessness</td>\n",
       "      <td>[159, 174, 213, 649, 774, 1447, 2370, 2406, 25...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.420270</td>\n",
       "      <td>0.333483</td>\n",
       "      <td>0.330385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>recovery work</td>\n",
       "      <td>[118, 296, 320, 367, 711, 763, 2219, 2520, 268...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.380811</td>\n",
       "      <td>0.221321</td>\n",
       "      <td>0.315276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>other people</td>\n",
       "      <td>[405, 621, 696, 957, 958, 1078, 1281, 1381, 14...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.327417</td>\n",
       "      <td>0.305706</td>\n",
       "      <td>0.136230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>relational healing</td>\n",
       "      <td>[137, 537, 578, 579, 581, 596, 604, 667, 1169,...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.255976</td>\n",
       "      <td>0.176727</td>\n",
       "      <td>0.233413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>effort</td>\n",
       "      <td>[298, 730, 1619, 1673, 1714, 1841, 1883, 2260,...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.529550</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>0.230758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>face</td>\n",
       "      <td>[402, 456, 1032, 1323, 1370, 1506, 1996, 2000,...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.446577</td>\n",
       "      <td>0.431832</td>\n",
       "      <td>0.205263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>dysfunctional family</td>\n",
       "      <td>[253, 274, 446, 517, 785, 1081, 2288, 2796, 30...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.435796</td>\n",
       "      <td>0.280180</td>\n",
       "      <td>0.340008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>substance</td>\n",
       "      <td>[515, 926, 927, 928, 931, 985, 1166, 1519, 285...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.413423</td>\n",
       "      <td>0.287688</td>\n",
       "      <td>0.244600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>share</td>\n",
       "      <td>[57, 1102, 1955, 1962, 2041, 2424, 2450, 2767,...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.614715</td>\n",
       "      <td>0.670420</td>\n",
       "      <td>0.251396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>issue</td>\n",
       "      <td>[118, 198, 284, 599, 1141, 2441, 2797, 2995, 3...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.503514</td>\n",
       "      <td>0.537838</td>\n",
       "      <td>0.375430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>critic attack</td>\n",
       "      <td>[765, 1688, 1770, 1935, 2266, 2292, 2430, 2539...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.640961</td>\n",
       "      <td>0.684384</td>\n",
       "      <td>0.181664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>back</td>\n",
       "      <td>[213, 653, 904, 943, 948, 1283, 1298, 1451, 15...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.376276</td>\n",
       "      <td>0.334985</td>\n",
       "      <td>0.230292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>real intimacy</td>\n",
       "      <td>[555, 571, 843, 980, 1039, 1043, 1353, 2039, 2...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.430390</td>\n",
       "      <td>0.312613</td>\n",
       "      <td>0.258380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>epiphany</td>\n",
       "      <td>[350, 803, 975, 1215, 1356, 1992, 1997, 2197, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.474505</td>\n",
       "      <td>0.502703</td>\n",
       "      <td>0.213307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>couple</td>\n",
       "      <td>[453, 602, 676, 710, 2681, 2883, 3169, 3187, 3...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.622853</td>\n",
       "      <td>0.835435</td>\n",
       "      <td>0.362369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>flashback management</td>\n",
       "      <td>[192, 749, 750, 781, 1504, 1526, 1574, 1587, 2...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.430090</td>\n",
       "      <td>0.454955</td>\n",
       "      <td>0.248909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2634</th>\n",
       "      <td>such time</td>\n",
       "      <td>[755, 760, 1132, 1515, 1890, 2012, 2021, 2268,...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.539580</td>\n",
       "      <td>0.585886</td>\n",
       "      <td>0.214290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255</th>\n",
       "      <td>isolation</td>\n",
       "      <td>[994, 1157, 1158, 1159, 1247, 1942, 2014, 2020...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.471471</td>\n",
       "      <td>0.374474</td>\n",
       "      <td>0.149767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>rage</td>\n",
       "      <td>[148, 159, 204, 1095, 1096, 2167, 2212, 2301, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.419586</td>\n",
       "      <td>0.329129</td>\n",
       "      <td>0.316507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>flashback management step</td>\n",
       "      <td>[192, 486, 767, 1371, 1469, 1522, 1762, 2414, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.423223</td>\n",
       "      <td>0.441141</td>\n",
       "      <td>0.236179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>cycle</td>\n",
       "      <td>[91, 1320, 2566, 2571, 2606, 2699, 2708, 2723,...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.667968</td>\n",
       "      <td>0.782583</td>\n",
       "      <td>0.259493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>advice</td>\n",
       "      <td>[122, 1333, 1341, 1349, 1408, 1903, 2525, 2990...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.533066</td>\n",
       "      <td>0.422823</td>\n",
       "      <td>0.264846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3121</th>\n",
       "      <td>fight response</td>\n",
       "      <td>[936, 1032, 1283, 1377, 1824, 1825, 1830, 1877...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.481181</td>\n",
       "      <td>0.547748</td>\n",
       "      <td>0.135955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>latter</td>\n",
       "      <td>[192, 200, 995, 1517, 1587, 1618, 2046, 2657, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.467100</td>\n",
       "      <td>0.476577</td>\n",
       "      <td>0.285465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>loneliness</td>\n",
       "      <td>[185, 378, 385, 790, 796, 987, 2151, 2322, 2461]</td>\n",
       "      <td>9</td>\n",
       "      <td>0.348849</td>\n",
       "      <td>0.239039</td>\n",
       "      <td>0.254951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Concept  \\\n",
       "193                      parent   \n",
       "530                       child   \n",
       "220                      critic   \n",
       "20                    flashback   \n",
       "4                      survivor   \n",
       "349                        fear   \n",
       "94                      feeling   \n",
       "61                       client   \n",
       "28                         life   \n",
       "313                    recovery   \n",
       "149                   childhood   \n",
       "670                       shame   \n",
       "55                        cptsd   \n",
       "159                       anger   \n",
       "49                    therapist   \n",
       "50                         year   \n",
       "46                  abandonment   \n",
       "125                        work   \n",
       "579                  depression   \n",
       "1472                       tear   \n",
       "969                      danger   \n",
       "601             image equation4   \n",
       "40          emotional flashback   \n",
       "192                        love   \n",
       "458                      friend   \n",
       "3704                   new york   \n",
       "208                      family   \n",
       "1618               inner critic   \n",
       "182                        pain   \n",
       "1263                    ability   \n",
       "...                         ...   \n",
       "1326                    mistake   \n",
       "758                hopelessness   \n",
       "2232                       hurt   \n",
       "992               fawn response   \n",
       "913                      victim   \n",
       "702                helplessness   \n",
       "555               recovery work   \n",
       "1610               other people   \n",
       "638          relational healing   \n",
       "1241                     effort   \n",
       "1597                       face   \n",
       "1103       dysfunctional family   \n",
       "1967                  substance   \n",
       "131                       share   \n",
       "559                       issue   \n",
       "2662              critic attack   \n",
       "936                        back   \n",
       "2102              real intimacy   \n",
       "1385                   epiphany   \n",
       "1734                     couple   \n",
       "827        flashback management   \n",
       "2634                  such time   \n",
       "3255                  isolation   \n",
       "667                        rage   \n",
       "822   flashback management step   \n",
       "410                       cycle   \n",
       "594                      advice   \n",
       "3121             fight response   \n",
       "825                      latter   \n",
       "795                  loneliness   \n",
       "\n",
       "                                              Occurence  Frequency      Mean  \\\n",
       "193   [85, 113, 134, 136, 144, 163, 213, 214, 217, 2...        178  0.409674   \n",
       "530   [113, 116, 143, 147, 203, 204, 205, 206, 208, ...        172  0.366698   \n",
       "220   [87, 118, 434, 435, 436, 469, 513, 515, 554, 6...        141  0.572694   \n",
       "20    [1, 146, 150, 159, 161, 163, 173, 178, 191, 19...        139  0.477972   \n",
       "4     [0, 95, 136, 147, 188, 191, 193, 199, 201, 295...        133  0.460261   \n",
       "349   [87, 97, 161, 173, 213, 214, 239, 250, 332, 34...        125  0.563133   \n",
       "94    [30, 113, 172, 193, 201, 296, 355, 371, 385, 3...        104  0.529106   \n",
       "61    [17, 101, 125, 389, 394, 519, 537, 587, 589, 5...        101  0.582511   \n",
       "28    [4, 9, 30, 35, 38, 56, 66, 67, 149, 187, 188, ...         85  0.400512   \n",
       "313   [87, 97, 320, 360, 504, 520, 536, 579, 593, 60...         79  0.443844   \n",
       "149   [64, 118, 121, 185, 203, 302, 304, 313, 363, 4...         77  0.500581   \n",
       "670   [148, 159, 162, 173, 213, 406, 567, 569, 570, ...         73  0.582134   \n",
       "55    [15, 87, 126, 127, 134, 138, 139, 143, 145, 14...         72  0.320938   \n",
       "159   [70, 278, 308, 376, 379, 391, 428, 601, 836, 9...         66  0.582806   \n",
       "49    [12, 22, 62, 135, 192, 193, 354, 467, 568, 578...         58  0.609729   \n",
       "50    [12, 34, 39, 62, 75, 80, 100, 101, 104, 114, 1...         57  0.354270   \n",
       "46    [11, 140, 141, 147, 173, 191, 202, 214, 334, 5...         54  0.472339   \n",
       "125   [54, 56, 66, 101, 124, 256, 325, 326, 376, 412...         52  0.479435   \n",
       "579   [121, 148, 379, 767, 769, 774, 836, 871, 943, ...         51  0.645428   \n",
       "1472  [376, 394, 404, 456, 481, 504, 565, 567, 996, ...         49  0.540075   \n",
       "969   [220, 224, 336, 382, 448, 804, 889, 909, 949, ...         47  0.518516   \n",
       "601   [124, 138, 300, 354, 362, 401, 407, 435, 441, ...         47  0.507584   \n",
       "40    [9, 44, 60, 146, 147, 149, 151, 157, 162, 172,...         46  0.362795   \n",
       "192   [85, 385, 402, 467, 494, 522, 543, 576, 606, 6...         45  0.540674   \n",
       "458   [97, 115, 127, 135, 281, 354, 390, 419, 467, 4...         45  0.506894   \n",
       "3704  [1179, 3248, 3249, 3250, 3252, 3256, 3257, 326...         44  0.973778   \n",
       "208   [85, 104, 115, 136, 142, 169, 172, 230, 231, 2...         43  0.319352   \n",
       "1618  [406, 435, 500, 724, 765, 887, 944, 1185, 1445...         43  0.568434   \n",
       "182   [83, 85, 231, 233, 415, 425, 428, 429, 755, 78...         42  0.501852   \n",
       "1263  [308, 358, 377, 382, 387, 437, 478, 479, 482, ...         41  0.378034   \n",
       "...                                                 ...        ...       ...   \n",
       "1326  [331, 644, 1461, 1693, 1799, 2213, 2716, 2743,...         10  0.585796   \n",
       "758   [174, 1022, 1447, 1689, 1764, 2406, 2561, 2562...         10  0.590571   \n",
       "2232  [601, 943, 1097, 1447, 2213, 2244, 2375, 2743,...         10  0.598048   \n",
       "992   [227, 958, 1038, 1146, 1272, 1277, 1300, 1305,...         10  0.337748   \n",
       "913   [207, 879, 1318, 1328, 1724, 1765, 2881, 2974,...         10  0.577357   \n",
       "702   [159, 174, 213, 649, 774, 1447, 2370, 2406, 25...         10  0.420270   \n",
       "555   [118, 296, 320, 367, 711, 763, 2219, 2520, 268...         10  0.380811   \n",
       "1610  [405, 621, 696, 957, 958, 1078, 1281, 1381, 14...         10  0.327417   \n",
       "638   [137, 537, 578, 579, 581, 596, 604, 667, 1169,...         10  0.255976   \n",
       "1241  [298, 730, 1619, 1673, 1714, 1841, 1883, 2260,...         10  0.529550   \n",
       "1597  [402, 456, 1032, 1323, 1370, 1506, 1996, 2000,...         10  0.446577   \n",
       "1103  [253, 274, 446, 517, 785, 1081, 2288, 2796, 30...         10  0.435796   \n",
       "1967  [515, 926, 927, 928, 931, 985, 1166, 1519, 285...         10  0.413423   \n",
       "131   [57, 1102, 1955, 1962, 2041, 2424, 2450, 2767,...         10  0.614715   \n",
       "559   [118, 198, 284, 599, 1141, 2441, 2797, 2995, 3...         10  0.503514   \n",
       "2662  [765, 1688, 1770, 1935, 2266, 2292, 2430, 2539...         10  0.640961   \n",
       "936   [213, 653, 904, 943, 948, 1283, 1298, 1451, 15...         10  0.376276   \n",
       "2102  [555, 571, 843, 980, 1039, 1043, 1353, 2039, 2...         10  0.430390   \n",
       "1385  [350, 803, 975, 1215, 1356, 1992, 1997, 2197, ...         10  0.474505   \n",
       "1734  [453, 602, 676, 710, 2681, 2883, 3169, 3187, 3...         10  0.622853   \n",
       "827   [192, 749, 750, 781, 1504, 1526, 1574, 1587, 2...         10  0.430090   \n",
       "2634  [755, 760, 1132, 1515, 1890, 2012, 2021, 2268,...         10  0.539580   \n",
       "3255  [994, 1157, 1158, 1159, 1247, 1942, 2014, 2020...          9  0.471471   \n",
       "667   [148, 159, 204, 1095, 1096, 2167, 2212, 2301, ...          9  0.419586   \n",
       "822   [192, 486, 767, 1371, 1469, 1522, 1762, 2414, ...          9  0.423223   \n",
       "410   [91, 1320, 2566, 2571, 2606, 2699, 2708, 2723,...          9  0.667968   \n",
       "594   [122, 1333, 1341, 1349, 1408, 1903, 2525, 2990...          9  0.533066   \n",
       "3121  [936, 1032, 1283, 1377, 1824, 1825, 1830, 1877...          9  0.481181   \n",
       "825   [192, 200, 995, 1517, 1587, 1618, 2046, 2657, ...          9  0.467100   \n",
       "795    [185, 378, 385, 790, 796, 987, 2151, 2322, 2461]          9  0.348849   \n",
       "\n",
       "        Median      Sdev  \n",
       "193   0.416066  0.268445  \n",
       "530   0.286937  0.260058  \n",
       "220   0.567568  0.200868  \n",
       "20    0.460661  0.242088  \n",
       "4     0.457357  0.272234  \n",
       "349   0.686486  0.274024  \n",
       "94    0.632883  0.304308  \n",
       "61    0.703003  0.275223  \n",
       "28    0.413213  0.291423  \n",
       "313   0.375976  0.274904  \n",
       "149   0.461261  0.281293  \n",
       "670   0.703003  0.272440  \n",
       "55    0.231682  0.293434  \n",
       "159   0.645946  0.235659  \n",
       "49    0.820871  0.319280  \n",
       "50    0.273574  0.304698  \n",
       "46    0.434835  0.295988  \n",
       "125   0.465916  0.320579  \n",
       "579   0.777778  0.243217  \n",
       "1472  0.600601  0.246311  \n",
       "969   0.493093  0.273745  \n",
       "601   0.549550  0.296061  \n",
       "40    0.366967  0.270388  \n",
       "192   0.573574  0.318084  \n",
       "458   0.532733  0.315418  \n",
       "3704  0.988739  0.094771  \n",
       "208   0.260661  0.284193  \n",
       "1618  0.580480  0.218128  \n",
       "182   0.580030  0.271745  \n",
       "1263  0.260661  0.255495  \n",
       "...        ...       ...  \n",
       "1326  0.602402  0.268661  \n",
       "758   0.626126  0.263755  \n",
       "2232  0.669219  0.261023  \n",
       "992   0.382733  0.097885  \n",
       "913   0.523874  0.292762  \n",
       "702   0.333483  0.330385  \n",
       "555   0.221321  0.315276  \n",
       "1610  0.305706  0.136230  \n",
       "638   0.176727  0.233413  \n",
       "1241  0.533784  0.230758  \n",
       "1597  0.431832  0.205263  \n",
       "1103  0.280180  0.340008  \n",
       "1967  0.287688  0.244600  \n",
       "131   0.670420  0.251396  \n",
       "559   0.537838  0.375430  \n",
       "2662  0.684384  0.181664  \n",
       "936   0.334985  0.230292  \n",
       "2102  0.312613  0.258380  \n",
       "1385  0.502703  0.213307  \n",
       "1734  0.835435  0.362369  \n",
       "827   0.454955  0.248909  \n",
       "2634  0.585886  0.214290  \n",
       "3255  0.374474  0.149767  \n",
       "667   0.329129  0.316507  \n",
       "822   0.441141  0.236179  \n",
       "410   0.782583  0.259493  \n",
       "594   0.422823  0.264846  \n",
       "3121  0.547748  0.135955  \n",
       "825   0.476577  0.285465  \n",
       "795   0.239039  0.254951  \n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conceptdata.sort_values(by='Frequency',ascending=False).head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conceptdata.to_csv('CPTSD'+'.csv',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dictionaries and dataframe to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# filename = 'A Mind For Numbers_ How to Excel at Math and Science (Even If You Flunked Algebra)'\n",
    "#filename = 'wikipedia4_wikihow3_no_wdlist'\n",
    "#filename = 'classical_mech_wiki'\n",
    "#filename = 'animal_kingdom_wiki'\n",
    "filename= 'CPTSD'\n",
    "concepts = {'sents':sents,'rawtxt':rawtxt,'sent_to_npflat':sent_to_npflat,'sent_to_tags':sent_to_tags,'sent_to_ltags':sent_to_ltags,'np_to_sent':np_to_sent,'Conceptdata':Conceptdata,'inv_eqn_dict':inv_eqn_dict}\n",
    "with open('../processed_data/'+filename+'concepts.pickle', 'wb') as f:\n",
    "    pickle.dump(concepts, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CPTSD'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rawtxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title : Complex PTSD: From Surviving to Thriving: A GUIDE AND MAP FOR RECOVERING FROM CHILDHOOD TRAUMA\\nAuthor: Walker, Pete\\n\\nGenerate From: eCore v0',\n",
       " '9',\n",
       " '12',\n",
       " '751 [ http://www',\n",
       " 'epubor',\n",
       " 'com/ecore',\n",
       " 'html ]\\n\\nTechnology Website: http://www',\n",
       " 'epubor',\n",
       " 'com\\n\\n\\n\\n [image file=image8',\n",
       " 'jpg]  [image file=image17',\n",
       " 'jpg] Testimonials about Pete Walker’s first book, The Tao of Fully Feeling, and his website: www',\n",
       " 'pete-walker',\n",
       " 'comI am writing from Survivors of Abuse Recovering (S',\n",
       " 'O',\n",
       " 'A',\n",
       " 'R',\n",
       " ') Society, located in Canada',\n",
       " ' We would like to include “13 Steps for Managing Flashbacks” in our resource manual',\n",
       " 'I found myself',\n",
       " ' I found myself in your words',\n",
       " ' It’s as if you had unzipped me, stepped inside my traumatized inner self, meandered around a bit, come back outside, and wrote about what you discovered inside of me',\n",
       " ' For the first time in my life',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'and I’m in my fifties now',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'I don’t feel defective',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'or crazy',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'or “weird”',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'or even unlovable',\n",
       " ' — D',\n",
       " 'M',\n",
       " 'I sat in the San Francisco Airport reading your book (in the washroom, shaking and weeping) to get the courage to go the next leg of the trip',\n",
       " ' It helped me so much just to know that you live in that area-strange when I haven’t even met you! Your website and book are invaluable to me',\n",
       " ' — A',\n",
       " ' R',\n",
       " 'I want to thank you so much for all the help you have given me (and all the people I’ve passed your website link onto since finding out about it)',\n",
       " ' Your understanding of emotional flashbacks has made an enormous difference in my life',\n",
       " ' I’ve gone from being smashed about by huge waves to having a surfboard on which I can ride at least some of them, and even if I fall off into it, I know it won’t last forever',\n",
       " ' — J, New ZealandThank you for all of your educational information with regards to PTSD and abandonment',\n",
       " ' I have finally found something that I have tried to explain to therapists for years',\n",
       " ' Every single piece of information is exactly what I experience from my PTSD and attachment depression',\n",
       " ' — AI thank you on a personal and professional level',\n",
       " ' Your articles on healing from CPTSD have excited me and validated me both',\n",
       " ' I will be a better therapist now, and heal further in my own life',\n",
       " ' — DYour article will be one of my regular handouts now to my clients',\n",
       " ' Needless to say I feel this information and the way you articulate it is a life saver! — L',\n",
       " 'PHow impactful all you have written has been for me and how much healing I have found in the pages of your website',\n",
       " ' Like the authors you note in your article on bibliotherapy - I was convinced you would have empathy for me had I the occasion to meet you - and here, in this moment, that belief is powerfully actualized',\n",
       " ' — J',\n",
       " 'S',\n",
       " 'I have been labeled and diagnosed with everything from panic disorder to separation anxiety and attachment disorder, bipolar disorder, generalized anxiety, etc',\n",
       " ' Then I found a therapist who said I had PTSD from long-term emotional abuse from my father and emotional neglect from my mother and that’s when things really started to click',\n",
       " ' I feel like everything I have been reading from this website is the final piece to the puzzle that I have been searching for in my journey',\n",
       " ' It is indeed very empowering and liberating',\n",
       " ' — A',\n",
       " 'M',\n",
       " 'I’m a long way into my own recovery process now and have recently reached a point of wanting to look back and celebrate how far I’ve come',\n",
       " ' Your words were just what I needed to see at this time',\n",
       " ' I feel really seen and understood and appreciated',\n",
       " ' What a gift',\n",
       " ' — P',\n",
       " 'After a degree in psychology, training in counseling and decades of therapy this is the first time I’ve read something that describes my internal state! — F',\n",
       " 'K',\n",
       " 'I’ve been working with your book for a few years, and for the first time in my life I’m able to be myself and feel a full range of feelings - and my kids are starting to flower due to this hard work',\n",
       " ' So thank you',\n",
       " ' — N',\n",
       " 'A',\n",
       " 'I wanted to extend my gratitude for all the information you have made available on complex PTSD',\n",
       " ' Clearly the best resource on the internet',\n",
       " ' — J',\n",
       " 'C',\n",
       " 'I found your online articles about 5 years ago, and have consistently come back to them as I work through Complex PTSD with a wonderful therapist']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
