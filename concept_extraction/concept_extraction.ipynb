{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept extraction from text\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading text file into string \n",
    "\n",
    "### Option 1. Downloading a wikipedia article's text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'https://en.wikipedia.org/wiki/Facet'\n",
    "\n",
    "source = requests.get(url).text\n",
    "soup = BeautifulSoup(source,'lxml')\n",
    "\n",
    "\n",
    "text_set = soup.find_all(['p']) ## This will skip headings ('h2','h3') and lists that are made as links( 'li')\n",
    "text_list = [p1.get_text() for p1 in text_set]\n",
    "tags_list = [p1.name for p1 in text_set ]\n",
    "\n",
    "rawtxt = ''.join(text_list)\n",
    "\n",
    "print(\"length of material\")\n",
    "print(len(rawtxt))\n",
    "\n",
    "print(\"Sample of text\")\n",
    "print(rawtxt[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save rawtxt as is for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'facetwiki.txt'\n",
    "path_name = \"C:/Users/Arati/Documents/personal docs/python_introduction_course/textdata/\"\n",
    "with open(path_name + filename,\"a\",encoding=\"utf-8\") as myfile:\n",
    "    myfile.write(rawtxt)\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. Getting file from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Cognitive_Load_Theory.txt'\n",
    "path_name = \"C:/Users/Arati/Documents/personal docs/python_introduction_course/textdata/\"\n",
    "with open (path_name +filename, \"r\",encoding=\"utf-8\") as myfile:\n",
    "    rawtxt=myfile.read()\n",
    "myfile.close()\n",
    "#rawtxt = rawtxt.encode('ascii','ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting list of concepts:\n",
    "\n",
    "### Importing libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.chunk import *\n",
    "from nltk.chunk.util import *\n",
    "from nltk.chunk.regexp import *\n",
    "from nltk import Tree\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "import nltk\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in text 4484\n",
      "4484\n",
      "Sample of sentences:\n",
      "['Explorations in the Learning Sciences, Instructional Systems and Performance Technologies\\n\\nPreface\\n\\nWithout knowledge of human cognitive processes, instructional design is blind.', 'In\\xa0the absence of an appropriate framework to suggest instructional techniques, we\\nare likely to have difficulty explaining why instructional procedures do or do not\\nwork.', 'Lacking knowledge of human cognition, we would be left with no overarching\\nstructure linking disparate instructional processes and guiding procedures.', 'Unless\\nwe can appeal to the manner in which human cognitive structures are organised,\\nknown as human cognitive architecture, a rational justification for \\xadrecommending\\none instructional procedure over another is unlikely to be available.', 'At best, we\\nwould be restricted to using narrow, empirical grounds indicating that particular\\nprocedures seem to work.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(rawtxt)\n",
    " \n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "sents = tokenizer.tokenize(rawtxt)\n",
    "\n",
    "print(\"Number of sentences in text \"+str(len(sents)))\n",
    "print(len(sents))\n",
    "\n",
    "print(\"Sample of sentences:\")\n",
    "print(sents[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token handling functions: \n",
    "1. validchar(wrd): checks if token is a valid alphanumeric+hyphens word\n",
    "2. lemmatize_by_pos(tag) lemmatizes token by part of speech\n",
    "3. chunk_this(grammar_rule_key,sentence_tags) chunks a particular grammar rule key (see chunkrules)\n",
    "4. eqn_label: extracts equation terms and replaces all occurences in text with a textkey, which is then treated as a noun phrase. Also updates equation dictionary\n",
    "5. display_equation (displays equation term by key)\n",
    "6. chunker: chunks each sentence by each chunking rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validchar(wrd):\n",
    "    p = re.compile(r'[^0-9a-zA-Z_-]')\n",
    "    if p.search(wrd) is None:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def lemmatize_by_pos(tag):\n",
    "    token = tag[0].lower()\n",
    "    pos = tag[1]\n",
    "    if token in stop_words:\n",
    "        return (token,pos)\n",
    "    if pos.startswith('J'):\n",
    "        # adjective form\n",
    "        lemma = wnl.lemmatize(token,'s')\n",
    "    elif pos.startswith('N'):\n",
    "        # noun form\n",
    "        lemma = wnl.lemmatize(token,'n')\n",
    "    elif pos.startswith('R'):\n",
    "        # adverb\n",
    "        lemma = wnl.lemmatize(token,'r')\n",
    "    elif pos.startswith('V'):\n",
    "        lemma = wnl.lemmatize(token,'v')\n",
    "    else:\n",
    "        lemma = token\n",
    "    return (lemma,pos)\n",
    "\n",
    "global eqn_dict\n",
    "eqn_dict = {}\n",
    "global eqn_count\n",
    "eqn_count = 1\n",
    "\n",
    "def eqn_label(tokens):\n",
    "    global eqn_count\n",
    "    global eqn_dict\n",
    "    EQNlist = [wrd for wrd in tokens if not(wrd.isalnum()) and re.search(r'[\\[\\]\\{\\}\\+*^=_%$]',wrd) and len(wrd)>1 ]\n",
    "    ## replace queations with a label and save to equation dictionary\n",
    "    for eqn in EQNlist:\n",
    "        \n",
    "        if not(eqn in eqn_dict):\n",
    "            \n",
    "            eqn_dict[eqn] = ''.join(['equation',str(eqn_count)])\n",
    "            eqn_count = eqn_count + 1                          \n",
    "        else:    \n",
    "            tokens[tokens.index(eqn)] = eqn_dict[eqn]\n",
    "                  \n",
    "    return tokens\n",
    "\n",
    "global inv_eqn_dict\n",
    "inv_eqn_dict = dict([[value,key] for key,value in eqn_dict.items()])\n",
    "\n",
    "def display_equation(reptokens):\n",
    "    for wrd in reptokens:\n",
    "        if wrd in inv_eqn_dict:\n",
    "            reptokens[reptokens.index(wrd)] = inv_eqn_dict[wrd]\n",
    "    return reptokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up chunking rules:\n",
    "\n",
    "Chunking done in batches to enable overlapping tokens to be extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkrules = {}\n",
    "\n",
    "# Define chunking rules here:\n",
    "chunkrules['JJNP'] = r\"\"\"    \n",
    "    JJNP: {<RB.*>?<J.*>?<NN.*>{1,}}       \n",
    "\"\"\"\n",
    "## Examples: \"reusable contactless stored value smart card\"\n",
    "\n",
    "def chunk_this(grammar_rule_key,sentence_tags):\n",
    "    setlist = []\n",
    "    cp = nltk.RegexpParser(chunkrules[grammar_rule_key])\n",
    "    J = cp.parse(sentence_tags) \n",
    "    for i in range(len(J)):\n",
    "        if not(isinstance(J[i],tuple)):\n",
    "            if (J[i].label()==grammar_rule_key):\n",
    "                setlist.append((' '.join([J[i][j][0] for j in range(len(J[i])) if (validchar(J[i][j][0])==1)])))\n",
    "    setlist = list(set(setlist))\n",
    "    setlist = [wrd.lower() for wrd in setlist if len(wrd)>0]\n",
    "    return setlist\n",
    "\n",
    "def chunker(sentence_tags):\n",
    "    return [chunk_this(key,sentence_tags)  for key in chunkrules]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "sent_to_np = {}\n",
    "sent_to_ltags = {}\n",
    "sent_to_tags = {}\n",
    "\n",
    "for i in range(len(sents)):\n",
    "    tokens = word_tokenize(sents[i])\n",
    "    reptokens = eqn_label(tokens)\n",
    "    tags = nltk.pos_tag(reptokens)\n",
    "    lemmatags = [lemmatize_by_pos(t) for t in tags]\n",
    "    sent_to_np[i] = chunker(lemmatags)\n",
    "    sent_to_ltags[i] = lemmatags\n",
    "    sent_to_tags[i] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['learning science',\n",
       "  'instructional design',\n",
       "  'instructional system',\n",
       "  'exploration',\n",
       "  'cognitive process',\n",
       "  'performance technology preface without knowledge']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_to_np[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten lists of lists containing chunks of different rules, dictionary of np to sent\n",
    "import itertools\n",
    "sent_to_npflat = {}\n",
    "np_to_sent = {}\n",
    "for key in sent_to_np:\n",
    "    sent_to_npflat[key] = list(set((itertools.chain(*sent_to_np[key]))))  \n",
    "    for np in sent_to_npflat[key]:            \n",
    "        if np in np_to_sent:                           \n",
    "            np_to_sent[np].append(key)\n",
    "        else:                \n",
    "            np_to_sent[np]=[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe with some metrics:\n",
    "- Concept: concept phrase\n",
    "- Occurence: list of sentences in which the phrase occurs\n",
    "- Frequency: number of sentences in which the phrase occurs\n",
    "- Mean: average of sentence numbers in the text in which the phrase occurs normalized to number of sentences\n",
    "- Median: median of sentence numbers in the text in which the phrase occurs normalized to number of sentences. Lets us know if phrase occurs much more in the beginning of the text, or towards the end. can indicate how central the phrase is to the text. \n",
    "- Sdev: standard deviation of the sentences in which the phrase occurs (indicates the dispersion of the phrase in the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as num\n",
    "import pandas as pd\n",
    "Concept = pd.Series([key for (key,value) in np_to_sent.items()])\n",
    "Occurence = pd.Series([num.array(value) for (key,value) in np_to_sent.items()])\n",
    "Frequency = pd.Series([len(o) for o in Occurence])\n",
    "Mean= pd.Series([num.mean(o) for o in Occurence])/len(sents)\n",
    "Median = pd.Series([num.median(o) for o in Occurence])/len(sents)\n",
    "Sdev = pd.Series([num.std(o) for o in Occurence])/len(sents)\n",
    "Conceptdata = pd.DataFrame({'Concept':Concept,'Occurence':Occurence,'Frequency':Frequency,'Mean':Mean,'Median':Median,'Sdev':Sdev})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Occurence</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Sdev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>cognitive load</td>\n",
       "      <td>[15, 46, 47, 48, 133, 975, 979, 1150, 1169, 11...</td>\n",
       "      <td>433</td>\n",
       "      <td>0.572308</td>\n",
       "      <td>0.537244</td>\n",
       "      <td>0.253204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>learner</td>\n",
       "      <td>[46, 174, 191, 201, 277, 278, 279, 282, 285, 2...</td>\n",
       "      <td>420</td>\n",
       "      <td>0.647316</td>\n",
       "      <td>0.695027</td>\n",
       "      <td>0.229364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>information</td>\n",
       "      <td>[17, 18, 19, 25, 27, 40, 128, 133, 147, 148, 1...</td>\n",
       "      <td>407</td>\n",
       "      <td>0.455098</td>\n",
       "      <td>0.485281</td>\n",
       "      <td>0.289363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>example</td>\n",
       "      <td>[29, 58, 91, 94, 97, 99, 124, 129, 145, 151, 1...</td>\n",
       "      <td>276</td>\n",
       "      <td>0.555967</td>\n",
       "      <td>0.549175</td>\n",
       "      <td>0.256705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>memory</td>\n",
       "      <td>[40, 41, 44, 46, 436, 507, 863, 864, 866, 880,...</td>\n",
       "      <td>226</td>\n",
       "      <td>0.469096</td>\n",
       "      <td>0.296945</td>\n",
       "      <td>0.288290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>student</td>\n",
       "      <td>[69, 189, 307, 621, 676, 1217, 1293, 1295, 129...</td>\n",
       "      <td>211</td>\n",
       "      <td>0.605917</td>\n",
       "      <td>0.579393</td>\n",
       "      <td>0.213465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>problem</td>\n",
       "      <td>[6, 33, 212, 219, 227, 228, 232, 239, 256, 278...</td>\n",
       "      <td>181</td>\n",
       "      <td>0.511788</td>\n",
       "      <td>0.460303</td>\n",
       "      <td>0.267847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>sweller</td>\n",
       "      <td>[107, 201, 207, 241, 277, 282, 293, 486, 527, ...</td>\n",
       "      <td>173</td>\n",
       "      <td>0.551640</td>\n",
       "      <td>0.518510</td>\n",
       "      <td>0.217999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>chapter</td>\n",
       "      <td>[17, 20, 35, 36, 47, 48, 49, 54, 55, 56, 89, 1...</td>\n",
       "      <td>166</td>\n",
       "      <td>0.542654</td>\n",
       "      <td>0.593555</td>\n",
       "      <td>0.284864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>long-term memory</td>\n",
       "      <td>[40, 41, 43, 149, 329, 384, 387, 389, 391, 393...</td>\n",
       "      <td>163</td>\n",
       "      <td>0.330377</td>\n",
       "      <td>0.233720</td>\n",
       "      <td>0.287350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>element interactivity</td>\n",
       "      <td>[1197, 1205, 1209, 1210, 1211, 1213, 1216, 121...</td>\n",
       "      <td>161</td>\n",
       "      <td>0.684622</td>\n",
       "      <td>0.817797</td>\n",
       "      <td>0.244092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>result</td>\n",
       "      <td>[52, 242, 271, 285, 438, 440, 443, 448, 489, 6...</td>\n",
       "      <td>156</td>\n",
       "      <td>0.618430</td>\n",
       "      <td>0.657337</td>\n",
       "      <td>0.223830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>knowledge</td>\n",
       "      <td>[2, 6, 12, 13, 15, 20, 28, 30, 38, 43, 66, 84,...</td>\n",
       "      <td>149</td>\n",
       "      <td>0.305677</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.303154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>worked example</td>\n",
       "      <td>[288, 1395, 1396, 1679, 1846, 1950, 1951, 1954...</td>\n",
       "      <td>146</td>\n",
       "      <td>0.604579</td>\n",
       "      <td>0.496989</td>\n",
       "      <td>0.175042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>element</td>\n",
       "      <td>[870, 872, 874, 877, 878, 919, 927, 942, 989, ...</td>\n",
       "      <td>141</td>\n",
       "      <td>0.580072</td>\n",
       "      <td>0.604594</td>\n",
       "      <td>0.289634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>diagram</td>\n",
       "      <td>[637, 638, 683, 1514, 1549, 1557, 1805, 2167, ...</td>\n",
       "      <td>135</td>\n",
       "      <td>0.584372</td>\n",
       "      <td>0.592997</td>\n",
       "      <td>0.123532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>study</td>\n",
       "      <td>[79, 650, 1464, 1485, 1498, 1499, 1515, 1530, ...</td>\n",
       "      <td>132</td>\n",
       "      <td>0.619107</td>\n",
       "      <td>0.577944</td>\n",
       "      <td>0.210852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>effect</td>\n",
       "      <td>[55, 60, 272, 360, 509, 521, 634, 773, 1235, 1...</td>\n",
       "      <td>128</td>\n",
       "      <td>0.636517</td>\n",
       "      <td>0.626784</td>\n",
       "      <td>0.224292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>principle</td>\n",
       "      <td>[291, 292, 357, 358, 359, 360, 448, 483, 576, ...</td>\n",
       "      <td>128</td>\n",
       "      <td>0.384344</td>\n",
       "      <td>0.244425</td>\n",
       "      <td>0.279073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>text</td>\n",
       "      <td>[486, 537, 538, 540, 1353, 1356, 1549, 1620, 1...</td>\n",
       "      <td>121</td>\n",
       "      <td>0.577904</td>\n",
       "      <td>0.599242</td>\n",
       "      <td>0.134638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Concept  \\\n",
       "53           cognitive load   \n",
       "143                 learner   \n",
       "65              information   \n",
       "106                 example   \n",
       "132                  memory   \n",
       "203                 student   \n",
       "30                  problem   \n",
       "308                 sweller   \n",
       "64                  chapter   \n",
       "131        long-term memory   \n",
       "1898  element interactivity   \n",
       "156                  result   \n",
       "14                knowledge   \n",
       "630          worked example   \n",
       "1494                element   \n",
       "1234                diagram   \n",
       "229                   study   \n",
       "163                  effect   \n",
       "638               principle   \n",
       "986                    text   \n",
       "\n",
       "                                              Occurence  Frequency      Mean  \\\n",
       "53    [15, 46, 47, 48, 133, 975, 979, 1150, 1169, 11...        433  0.572308   \n",
       "143   [46, 174, 191, 201, 277, 278, 279, 282, 285, 2...        420  0.647316   \n",
       "65    [17, 18, 19, 25, 27, 40, 128, 133, 147, 148, 1...        407  0.455098   \n",
       "106   [29, 58, 91, 94, 97, 99, 124, 129, 145, 151, 1...        276  0.555967   \n",
       "132   [40, 41, 44, 46, 436, 507, 863, 864, 866, 880,...        226  0.469096   \n",
       "203   [69, 189, 307, 621, 676, 1217, 1293, 1295, 129...        211  0.605917   \n",
       "30    [6, 33, 212, 219, 227, 228, 232, 239, 256, 278...        181  0.511788   \n",
       "308   [107, 201, 207, 241, 277, 282, 293, 486, 527, ...        173  0.551640   \n",
       "64    [17, 20, 35, 36, 47, 48, 49, 54, 55, 56, 89, 1...        166  0.542654   \n",
       "131   [40, 41, 43, 149, 329, 384, 387, 389, 391, 393...        163  0.330377   \n",
       "1898  [1197, 1205, 1209, 1210, 1211, 1213, 1216, 121...        161  0.684622   \n",
       "156   [52, 242, 271, 285, 438, 440, 443, 448, 489, 6...        156  0.618430   \n",
       "14    [2, 6, 12, 13, 15, 20, 28, 30, 38, 43, 66, 84,...        149  0.305677   \n",
       "630   [288, 1395, 1396, 1679, 1846, 1950, 1951, 1954...        146  0.604579   \n",
       "1494  [870, 872, 874, 877, 878, 919, 927, 942, 989, ...        141  0.580072   \n",
       "1234  [637, 638, 683, 1514, 1549, 1557, 1805, 2167, ...        135  0.584372   \n",
       "229   [79, 650, 1464, 1485, 1498, 1499, 1515, 1530, ...        132  0.619107   \n",
       "163   [55, 60, 272, 360, 509, 521, 634, 773, 1235, 1...        128  0.636517   \n",
       "638   [291, 292, 357, 358, 359, 360, 448, 483, 576, ...        128  0.384344   \n",
       "986   [486, 537, 538, 540, 1353, 1356, 1549, 1620, 1...        121  0.577904   \n",
       "\n",
       "        Median      Sdev  \n",
       "53    0.537244  0.253204  \n",
       "143   0.695027  0.229364  \n",
       "65    0.485281  0.289363  \n",
       "106   0.549175  0.256705  \n",
       "132   0.296945  0.288290  \n",
       "203   0.579393  0.213465  \n",
       "30    0.460303  0.267847  \n",
       "308   0.518510  0.217999  \n",
       "64    0.593555  0.284864  \n",
       "131   0.233720  0.287350  \n",
       "1898  0.817797  0.244092  \n",
       "156   0.657337  0.223830  \n",
       "14    0.169492  0.303154  \n",
       "630   0.496989  0.175042  \n",
       "1494  0.604594  0.289634  \n",
       "1234  0.592997  0.123532  \n",
       "229   0.577944  0.210852  \n",
       "163   0.626784  0.224292  \n",
       "638   0.244425  0.279073  \n",
       "986   0.599242  0.134638  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conceptdata.sort_values(by='Frequency',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conceptdata.to_csv(filename[0:-4]+'.csv',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dictionaries and dataframe to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "concepts = {'sent_to_npflat':sent_to_npflat,'sent_to_tags':sent_to_tags,'sent_to_ltags':sent_to_ltags,'np_to_sent':np_to_sent,'Conceptdata':Conceptdata}\n",
    "with open(filename[0:-4]+'concepts.pickle', 'wb') as f:\n",
    "    pickle.dump(concepts, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
