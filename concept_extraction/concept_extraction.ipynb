{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept extraction from text\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading text file into string \n",
    "\n",
    "### Option 1. Downloading a wikipedia article's text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of material\n",
      "59186\n",
      "Sample of text\n",
      "\n",
      "\n",
      "A star is an astronomical object consisting of a luminous spheroid of plasma held together by its own gravity. The nearest star to Earth is the Sun. Many other stars are visible to the naked eye from Earth during the night, appearing as a multitude of fixed luminous points in the sky due to their immense distance from Earth. Historically, the most prominent stars were grouped into constellations and asterisms, the brightest of which gained proper names. Astronomers have assembled star catalogu\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'https://en.wikipedia.org/wiki/Star'\n",
    "\n",
    "source = requests.get(url).text\n",
    "soup = BeautifulSoup(source,'lxml')\n",
    "\n",
    "\n",
    "text_set = soup.find_all(['p']) ## This will skip headings ('h2','h3') and lists that are made as links( 'li')\n",
    "text_list = [p1.get_text() for p1 in text_set]\n",
    "tags_list = [p1.name for p1 in text_set ]\n",
    "\n",
    "rawtxt = ''.join(text_list)\n",
    "\n",
    "print(\"length of material\")\n",
    "print(len(rawtxt))\n",
    "\n",
    "print(\"Sample of text\")\n",
    "print(rawtxt[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save rawtxt as is for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'starwiki.txt'\n",
    "path_name = \"C:/Users/Arati/Documents/personal docs/python_introduction_course/textdata/\"\n",
    "with open(path_name + filename,\"a\",encoding=\"utf-8\") as myfile:\n",
    "    myfile.write(rawtxt)\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. Getting file from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Cognitive_Load_Theory.txt'\n",
    "path_name = \"C:/Users/Arati/Documents/personal docs/python_introduction_course/textdata/\"\n",
    "with open (path_name +filename, \"r\",encoding=\"utf-8\") as myfile:\n",
    "    rawtxt=myfile.read()\n",
    "myfile.close()\n",
    "#rawtxt = rawtxt.encode('ascii','ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting list of concepts:\n",
    "\n",
    "### Importing libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.chunk import *\n",
    "from nltk.chunk.util import *\n",
    "from nltk.chunk.regexp import *\n",
    "from nltk import Tree\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "import nltk\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting\n",
    "#### Option 1: split on periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = rawtxt.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Pretrained NLTK sentence splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.sent_tokenize(rawtxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Train unsupervised on domain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in text 423\n",
      "423\n",
      "Sample of sentences:\n",
      "['\\n\\nA star is an astronomical object consisting of a luminous spheroid of plasma held together by its own gravity.', 'The nearest star to Earth is the Sun.', 'Many other stars are visible to the naked eye from Earth during the night, appearing as a multitude of fixed luminous points in the sky due to their immense distance from Earth.', 'Historically, the most prominent stars were grouped into constellations and asterisms, the brightest of which gained proper names.', 'Astronomers have assembled star catalogues that identify the known stars and provide standardized stellar designations.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(rawtxt)\n",
    " \n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "sents = tokenizer.tokenize(rawtxt)\n",
    "\n",
    "print(\"Number of sentences in text \"+str(len(sents)))\n",
    "print(len(sents))\n",
    "\n",
    "print(\"Sample of sentences:\")\n",
    "print(sents[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token handling functions: \n",
    "1. validchar(wrd): checks if token is a valid alphanumeric+hyphens word\n",
    "2. lemmatize_by_pos(tag) lemmatizes token by part of speech\n",
    "3. chunk_this(grammar_rule_key,sentence_tags) chunks a particular grammar rule key (see chunkrules)\n",
    "4. eqn_label: extracts equation terms and replaces all occurences in text with a textkey, which is then treated as a noun phrase. Also updates equation dictionary\n",
    "5. display_equation (displays equation term by key)\n",
    "6. chunker: chunks each sentence by each chunking rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validchar(wrd):\n",
    "    p = re.compile(r'[^0-9a-zA-Z_-]')\n",
    "    if p.search(wrd) is None:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def lemmatize_by_pos(tag):\n",
    "    token = tag[0].lower()\n",
    "    pos = tag[1]\n",
    "    if token in stop_words:\n",
    "        return (token,pos)\n",
    "    if pos.startswith('J'):\n",
    "        # adjective form\n",
    "        lemma = wnl.lemmatize(token,'s')\n",
    "    elif pos.startswith('N'):\n",
    "        # noun form\n",
    "        lemma = wnl.lemmatize(token,'n')\n",
    "    elif pos.startswith('R'):\n",
    "        # adverb\n",
    "        lemma = wnl.lemmatize(token,'r')\n",
    "    elif pos.startswith('V'):\n",
    "        lemma = wnl.lemmatize(token,'v')\n",
    "    else:\n",
    "        lemma = token\n",
    "    return (lemma,pos)\n",
    "\n",
    "global eqn_dict\n",
    "eqn_dict = {}\n",
    "global eqn_count\n",
    "eqn_count = 1\n",
    "\n",
    "def eqn_label(tokens):\n",
    "    global eqn_count\n",
    "    global eqn_dict\n",
    "    EQNlist = [wrd for wrd in tokens if not(wrd.isalnum()) and re.search(r'[\\[\\]\\{\\}\\+*^=_%$]',wrd) and len(wrd)>1 ]\n",
    "    ## replace queations with a label and save to equation dictionary\n",
    "    for eqn in EQNlist:\n",
    "        \n",
    "        if not(eqn in eqn_dict):\n",
    "            \n",
    "            eqn_dict[eqn] = ''.join(['equation',str(eqn_count)])\n",
    "            eqn_count = eqn_count + 1                          \n",
    "        else:    \n",
    "            tokens[tokens.index(eqn)] = eqn_dict[eqn]\n",
    "                  \n",
    "    return tokens\n",
    "\n",
    "global inv_eqn_dict\n",
    "inv_eqn_dict = dict([[value,key] for key,value in eqn_dict.items()])\n",
    "\n",
    "def display_equation(reptokens):\n",
    "    for wrd in reptokens:\n",
    "        if wrd in inv_eqn_dict:\n",
    "            reptokens[reptokens.index(wrd)] = inv_eqn_dict[wrd]\n",
    "    return reptokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up chunking rules:\n",
    "\n",
    "Chunking done in batches to enable overlapping tokens to be extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkrules = {}\n",
    "\n",
    "# Define chunking rules here:\n",
    "chunkrules['JJNP'] = r\"\"\"    \n",
    "    JJNP: {<RB.*>?<J.*>?<NN.*>{1,}}       \n",
    "\"\"\"\n",
    "## Examples: \"reusable contactless stored value smart card\"\n",
    "\n",
    "def chunk_this(grammar_rule_key,sentence_tags):\n",
    "    setlist = []\n",
    "    cp = nltk.RegexpParser(chunkrules[grammar_rule_key])\n",
    "    J = cp.parse(sentence_tags) \n",
    "    for i in range(len(J)):\n",
    "        if not(isinstance(J[i],tuple)):\n",
    "            if (J[i].label()==grammar_rule_key):\n",
    "                setlist.append((' '.join([J[i][j][0] for j in range(len(J[i])) if (validchar(J[i][j][0])==1)])))\n",
    "    setlist = list(set(setlist))\n",
    "    setlist = [wrd.lower() for wrd in setlist if len(wrd)>0]\n",
    "    return setlist\n",
    "\n",
    "def chunker(sentence_tags):\n",
    "    return [chunk_this(key,sentence_tags)  for key in chunkrules]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "sent_to_np = {}\n",
    "sent_to_ltags = {}\n",
    "sent_to_tags = {}\n",
    "\n",
    "for i in range(len(sents)):\n",
    "    tokens = word_tokenize(sents[i])\n",
    "    reptokens = eqn_label(tokens)\n",
    "    tags = nltk.pos_tag(reptokens)\n",
    "    lemmatags = [lemmatize_by_pos(t) for t in tags]\n",
    "    sent_to_np[i] = chunker(lemmatags)\n",
    "    sent_to_ltags[i] = lemmatags\n",
    "    sent_to_tags[i] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['luminous spheroid',\n",
       "  'astronomical object consisting',\n",
       "  'star',\n",
       "  'own gravity']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_to_np[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten lists of lists containing chunks of different rules, dictionary of np to sent\n",
    "import itertools\n",
    "sent_to_npflat = {}\n",
    "np_to_sent = {}\n",
    "for key in sent_to_np:\n",
    "    sent_to_npflat[key] = list(set((itertools.chain(*sent_to_np[key]))))  \n",
    "    for np in sent_to_npflat[key]:            \n",
    "        if np in np_to_sent:                           \n",
    "            np_to_sent[np].append(key)\n",
    "        else:                \n",
    "            np_to_sent[np]=[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe with some metrics:\n",
    "- Concept: concept phrase\n",
    "- Occurence: list of sentences in which the phrase occurs\n",
    "- Frequency: number of sentences in which the phrase occurs\n",
    "- Mean: average of sentence numbers in the text in which the phrase occurs normalized to number of sentences\n",
    "- Median: median of sentence numbers in the text in which the phrase occurs normalized to number of sentences. Lets us know if phrase occurs much more in the beginning of the text, or towards the end. can indicate how central the phrase is to the text. \n",
    "- Sdev: standard deviation of the sentences in which the phrase occurs (indicates the dispersion of the phrase in the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as num\n",
    "import pandas as pd\n",
    "Concept = pd.Series([key for (key,value) in np_to_sent.items()])\n",
    "Occurence = pd.Series([num.array(value) for (key,value) in np_to_sent.items()])\n",
    "Frequency = pd.Series([len(o) for o in Occurence])\n",
    "Mean= pd.Series([num.mean(o) for o in Occurence])/len(sents)\n",
    "Median = pd.Series([num.median(o) for o in Occurence])/len(sents)\n",
    "Sdev = pd.Series([num.std(o) for o in Occurence])/len(sents)\n",
    "Conceptdata = pd.DataFrame({'Concept':Concept,'Occurence':Occurence,'Frequency':Frequency,'Mean':Mean,'Median':Median,'Sdev':Sdev})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Occurence</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Sdev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>star</td>\n",
       "      <td>[0, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 17, 18,...</td>\n",
       "      <td>177</td>\n",
       "      <td>0.460608</td>\n",
       "      <td>0.480573</td>\n",
       "      <td>0.282031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sun</td>\n",
       "      <td>[1, 19, 29, 30, 31, 50, 51, 61, 103, 116, 121,...</td>\n",
       "      <td>46</td>\n",
       "      <td>0.537121</td>\n",
       "      <td>0.565440</td>\n",
       "      <td>0.291083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>core</td>\n",
       "      <td>[6, 16, 19, 20, 22, 129, 136, 150, 152, 181, 1...</td>\n",
       "      <td>35</td>\n",
       "      <td>0.539001</td>\n",
       "      <td>0.503067</td>\n",
       "      <td>0.305484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>time</td>\n",
       "      <td>[19, 54, 162, 177, 179, 183, 188, 207, 275, 29...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.619026</td>\n",
       "      <td>0.674847</td>\n",
       "      <td>0.243253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>mass</td>\n",
       "      <td>[9, 18, 21, 73, 116, 120, 132, 167, 175, 184, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.527765</td>\n",
       "      <td>0.564417</td>\n",
       "      <td>0.304928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>luminosity</td>\n",
       "      <td>[9, 12, 53, 116, 120, 131, 142, 152, 153, 249,...</td>\n",
       "      <td>22</td>\n",
       "      <td>0.544618</td>\n",
       "      <td>0.722904</td>\n",
       "      <td>0.306374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>year</td>\n",
       "      <td>[92, 97, 98, 138, 155, 158, 160, 163, 168, 169...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.410532</td>\n",
       "      <td>0.349693</td>\n",
       "      <td>0.161868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>helium</td>\n",
       "      <td>[6, 7, 14, 15, 84, 129, 150, 152, 170, 175, 17...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.408180</td>\n",
       "      <td>0.358896</td>\n",
       "      <td>0.313027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>temperature</td>\n",
       "      <td>[11, 12, 77, 135, 152, 171, 184, 187, 194, 333...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.552249</td>\n",
       "      <td>0.682004</td>\n",
       "      <td>0.292470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>earth</td>\n",
       "      <td>[1, 2, 5, 31, 88, 95, 121, 204, 239, 268, 269,...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.435222</td>\n",
       "      <td>0.488753</td>\n",
       "      <td>0.307340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>hydrogen</td>\n",
       "      <td>[6, 14, 84, 124, 127, 129, 150, 171, 181, 193,...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.397367</td>\n",
       "      <td>0.359918</td>\n",
       "      <td>0.256978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>energy</td>\n",
       "      <td>[6, 15, 16, 199, 200, 333, 341, 344, 345, 440,...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.643788</td>\n",
       "      <td>0.704499</td>\n",
       "      <td>0.346263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>galaxy</td>\n",
       "      <td>[5, 25, 45, 87, 90, 98, 132, 225, 226, 228, 24...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.349148</td>\n",
       "      <td>0.460123</td>\n",
       "      <td>0.208553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>astronomer</td>\n",
       "      <td>[4, 9, 29, 51, 71, 92, 93, 96, 132, 176, 287, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.300029</td>\n",
       "      <td>0.193252</td>\n",
       "      <td>0.249463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>massive star</td>\n",
       "      <td>[127, 131, 132, 141, 164, 194, 196, 198, 208, ...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.432515</td>\n",
       "      <td>0.398773</td>\n",
       "      <td>0.198425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>main sequence star</td>\n",
       "      <td>[129, 248, 317, 325, 332, 338, 411, 439, 453, ...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.753067</td>\n",
       "      <td>0.765849</td>\n",
       "      <td>0.210646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>rotation</td>\n",
       "      <td>[11, 140, 291, 297, 298, 320, 322, 325, 326, 3...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.555122</td>\n",
       "      <td>0.654397</td>\n",
       "      <td>0.198850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>heavy element</td>\n",
       "      <td>[14, 20, 21, 124, 179, 219, 220, 261, 262, 263...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.384272</td>\n",
       "      <td>0.447853</td>\n",
       "      <td>0.273390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>region</td>\n",
       "      <td>[123, 132, 133, 223, 288, 292, 449, 450, 452, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.614315</td>\n",
       "      <td>0.593047</td>\n",
       "      <td>0.280116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>spectrum</td>\n",
       "      <td>[9, 61, 85, 193, 266, 318, 336, 403, 413, 416]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.511247</td>\n",
       "      <td>0.597137</td>\n",
       "      <td>0.299216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Concept                                          Occurence  \\\n",
       "2                  star  [0, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 17, 18,...   \n",
       "5                   sun  [1, 19, 29, 30, 31, 50, 51, 61, 103, 116, 121,...   \n",
       "32                 core  [6, 16, 19, 20, 22, 129, 136, 150, 152, 181, 1...   \n",
       "82                 time  [19, 54, 162, 177, 179, 183, 188, 207, 275, 29...   \n",
       "44                 mass  [9, 18, 21, 73, 116, 120, 132, 167, 175, 184, ...   \n",
       "42           luminosity  [9, 12, 53, 116, 120, 131, 142, 152, 153, 249,...   \n",
       "350                year  [92, 97, 98, 138, 155, 158, 160, 163, 168, 169...   \n",
       "31               helium  [6, 7, 14, 15, 84, 129, 150, 152, 170, 175, 17...   \n",
       "61          temperature  [11, 12, 77, 135, 152, 171, 184, 187, 194, 333...   \n",
       "4                 earth  [1, 2, 5, 31, 88, 95, 121, 204, 239, 268, 269,...   \n",
       "35             hydrogen  [6, 14, 84, 124, 127, 129, 150, 171, 181, 193,...   \n",
       "34               energy  [6, 15, 16, 199, 200, 333, 341, 344, 345, 440,...   \n",
       "22               galaxy  [5, 25, 45, 87, 90, 98, 132, 225, 226, 228, 24...   \n",
       "20           astronomer  [4, 9, 29, 51, 71, 92, 93, 96, 132, 176, 287, ...   \n",
       "488        massive star  [127, 131, 132, 141, 164, 194, 196, 198, 208, ...   \n",
       "494  main sequence star  [129, 248, 317, 325, 332, 338, 411, 439, 453, ...   \n",
       "57             rotation  [11, 140, 291, 297, 298, 320, 322, 325, 326, 3...   \n",
       "69        heavy element  [14, 20, 21, 124, 179, 219, 220, 261, 262, 263...   \n",
       "475              region  [123, 132, 133, 223, 288, 292, 449, 450, 452, ...   \n",
       "50             spectrum     [9, 61, 85, 193, 266, 318, 336, 403, 413, 416]   \n",
       "\n",
       "     Frequency      Mean    Median      Sdev  \n",
       "2          177  0.460608  0.480573  0.282031  \n",
       "5           46  0.537121  0.565440  0.291083  \n",
       "32          35  0.539001  0.503067  0.305484  \n",
       "82          27  0.619026  0.674847  0.243253  \n",
       "44          26  0.527765  0.564417  0.304928  \n",
       "42          22  0.544618  0.722904  0.306374  \n",
       "350         20  0.410532  0.349693  0.161868  \n",
       "31          20  0.408180  0.358896  0.313027  \n",
       "61          20  0.552249  0.682004  0.292470  \n",
       "4           17  0.435222  0.488753  0.307340  \n",
       "35          16  0.397367  0.359918  0.256978  \n",
       "34          16  0.643788  0.704499  0.346263  \n",
       "22          15  0.349148  0.460123  0.208553  \n",
       "20          14  0.300029  0.193252  0.249463  \n",
       "488         12  0.432515  0.398773  0.198425  \n",
       "494         12  0.753067  0.765849  0.210646  \n",
       "57          11  0.555122  0.654397  0.198850  \n",
       "69          11  0.384272  0.447853  0.273390  \n",
       "475         10  0.614315  0.593047  0.280116  \n",
       "50          10  0.511247  0.597137  0.299216  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conceptdata.sort_values(by='Frequency',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conceptdata.to_csv(filename[0:-4]+'.csv',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dictionaries and dataframe to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "concepts = {'sents':sents,'rawtxt':rawtxt,'sent_to_npflat':sent_to_npflat,'sent_to_tags':sent_to_tags,'sent_to_ltags':sent_to_ltags,'np_to_sent':np_to_sent,'Conceptdata':Conceptdata}\n",
    "with open(filename[0:-4]+'concepts.pickle', 'wb') as f:\n",
    "    pickle.dump(concepts, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
