{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build IDF look up dictionary from a corpus\n",
    "\n",
    "Reuters corpus used below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "from nltk.corpus import reuters,brown\n",
    "cfileids = reuters.fileids() # list of filenames in reuters corpus\n",
    "wnl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a 1-gram dictionary of words and their document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique lemmatized words found:  26723  in a total of  10788  documents\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "document_freq = {} # Dictionary: key: word,  value:# of documents word occurs in \n",
    "\n",
    "for fname in cfileids:\n",
    "    corptext = [wrd.lower() for wrd in reuters.words(fname) if wrd.isalpha()]\n",
    "    corptextlemma = [wnl.lemmatize(wrd.lower()) for wrd in corptext]\n",
    "    corptextlemmaset = set(corptextlemma)\n",
    "    for wrd in corptextlemmaset:\n",
    "        document_freq[wrd] = document_freq.get(wrd,0) + 1 \n",
    "\n",
    "print('Total number of unique lemmatized words found: ',len(document_freq),' in a total of ',len(cfileids),' documents')\n",
    "\n",
    "totaldocs = len(cfileids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_dict = {} # dictionary: key: word, value: idf value in corpus\n",
    "for wrd in document_freq.keys():\n",
    "    idf_dict[wrd] = math.log(totaldocs) - math.log(1+document_freq[wrd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(wrd):\n",
    "    wrd = wrd.lower()\n",
    "    return idf_dict.get(wrd,math.log(totaldocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1355869158133407\n",
      "6.2904574107056295\n",
      "9.28618968425962\n"
     ]
    }
   ],
   "source": [
    "print(get_idf('long'))\n",
    "print(get_idf('memory'))\n",
    "print(get_idf('fun'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely need a bigger corpus for this, in science and technology domain. (Reuters is news articles)\n",
    "Plan to make idf dictionary based on wikidump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('idf_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(idf_dict, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
